{
  "documents": [
    {
      "id": "best-practices/ai-slop-antipatterns",
      "title": "AI Slop Antipatterns",
      "tags": "ai code-quality best-practices antipatterns llm writing code-review documentation",
      "content": "AI Slop Antipatterns Low-quality patterns to avoid when using AI-assisted coding and writing. &quot;Slop&quot; refers to AI-generated content that lacks substance, originality, or accuracy. Code Antipatterns Structural Issues Duplicate declarations - Same variable declared multiple times Phantom references - Using objects/modules that do not exist Standard library rewrites - Reimplementing functions that already exist in stdlib Unnecessary overloads - Overriding standard methods without reason Logic &amp; Quality Impossible-to-fail tests - Logic bugs that make tests always pass Swallowed exceptions - Empty or generic try/except that hides errors Over-engineering - Abstractions for one-time operations Edge case blindness - Missing validation at system boundaries Meaningless mocking - Tests that mock so much they test nothing Architecture Repetitive boilerplate - Copy-paste code that could be simplified Unclear organization - Poor file/module structure Context window pollution - Verbose code that wastes LLM context Premature abstraction - Helpers for code used once Writing/Documentation Antipatterns Hedging Phrases (remove these) &quot;It is important to note that...&quot; &quot;Arguably...&quot; &quot;To some extent...&quot; &quot;It should be noted that...&quot; &quot;In many cases...&quot; Structural Red Flags Generic numbered lists - Structure without substance Uniform tone - Suspiciously perfect, robotic consistency Rhythmic patterns - Sentences starting the same way repeatedly Surface insights - Observations without specific examples Filler content - Verbose explanations of obvious concepts Detection Heuristics Read aloud - Does it sound natural or robotic? Specificity check - Are there concrete examples or just abstractions? Utility test - Does this add value or just fill space? Variation scan - Is sentence structure varied or formulaic? Logic trace - Does the code actually do what it claims? Prevention Review AI output critically before accepting Ask &quot;what does this actually do?&quot; for each code block Prefer editing existing code over accepting AI rewrites Trust AI for explanation and boilerplate, not core logic Delete hedging phrases and filler on sight References Merriam-Webster 2025 Word of the Year: &quot;Slop&quot; AI-assisted code averages ~11 issues vs ~6 for human-only code Most public code is mediocre; AI trained on it produces mediocre output"
    },
    {
      "id": "tooling/claude-code-local-plugin-hooks-bug",
      "title": "Claude Code Local Plugin Hooks Bug",
      "tags": "tooling claude-code plugins hooks bugs workarounds",
      "content": "Claude Code Local Plugin Hooks Bug Local file-based marketplace plugins have their hooks matched but never executed. This affects SessionStart, PreCompact, UserPromptSubmit, and all other hook types. The Bug When plugins are registered via a local directory path (using &quot;source&quot;: &quot;directory&quot; in known_marketplaces.json ), their hooks are: Discovered - Claude Code finds the hooks in the plugin's plugin.json Matched - Debug logs show hooks matching queries like &quot;startup&quot; Never executed - Despite matching, the hook commands never run This differs from GitHub-sourced plugins which are cached to ~/.claude/plugins/marketplaces/ and have their hooks copied to the standard location. Affected Versions Claude Code 2.0.27+ All versions as of December 2025 (bugs still open) Root Cause Local plugins are referenced in-place and never copied to ~/.claude/plugins/*/hooks/hooks.json . The hook execution system only looks in that cached location, so local plugin hooks are discovered but skipped during execution. Workaround Add hooks directly to ~/.claude/settings.json instead of relying on the plugin's plugin.json : { &quot;hooks&quot;: { &quot;SessionStart&quot;: [ { &quot;matcher&quot;: &quot;&quot;, &quot;hooks&quot;: [ { &quot;type&quot;: &quot;command&quot;, &quot;command&quot;: &quot;your-plugin-command prime&quot; } ] } ], &quot;PreCompact&quot;: [ { &quot;matcher&quot;: &quot;&quot;, &quot;hooks&quot;: [ { &quot;type&quot;: &quot;command&quot;, &quot;command&quot;: &quot;your-plugin-command prime&quot; } ] } ] } } This defeats the purpose of distributable plugin hooks but works reliably. Related Issues #11509 - SessionStart hooks never execute for local file-based marketplace plugins (primary bug) #11939 - SessionStart hooks from local plugins (isLocal: true) match but don't execute #12151 - Plugin hook output not captured or passed to agent #11649 - SessionStart hook doesn't receive CLAUDE_ENV_FILE when installed by a plugin #10997 - SessionStart hooks don't execute on first run with GitHub marketplace plugins #12634 - Plugin SessionStart hook not automatically triggered Example: vl-mail Plugin The vl-mail plugin defines hooks in .claude-plugin/plugin.json : { &quot;hooks&quot;: { &quot;SessionStart&quot;: [ { &quot;matcher&quot;: &quot;&quot;, &quot;hooks&quot;: [ { &quot;type&quot;: &quot;command&quot;, &quot;command&quot;: &quot;vl-mail prime&quot; } ] } ] } } These hooks never fire. The workaround is adding to ~/.claude/settings.json : { &quot;hooks&quot;: { &quot;SessionStart&quot;: [ { &quot;matcher&quot;: &quot;&quot;, &quot;hooks&quot;: [ { &quot;type&quot;: &quot;command&quot;, &quot;command&quot;: &quot;vl-mail prime&quot; } ] } ] } } Debugging Enable debug mode to see hook matching: claude --debug Look for log lines like: Getting matching hook commands for SessionStart with query: startup Found N hook matchers in settings Matched N unique hooks for query 'startup' If hooks are matched but no execution output appears, you've hit this bug. Status As of December 2025, all related issues remain open on the Claude Code GitHub repository."
    },
    {
      "id": "tooling/beads-issue-tracker",
      "title": "Beads Issue Tracker",
      "tags": "tooling issue-tracking ai-agents git",
      "content": "Beads Issue Tracker A git-backed distributed issue tracker designed for AI-supervised coding workflows. Command: bd Repository Source: github.com/steveyegge/beads Location in voidlabs-ansible: /srv/fast/code/voidlabs-ansible/beads Core Concept Beads enables a distributed, git-backed issue tracker that feels like a centralized database through a three-layer architecture: SQLite DB (.beads/beads.db, gitignored) ↕ auto-sync (5s debounce) JSONL (.beads/issues.jsonl, git-tracked) ↕ git push/pull Remote JSONL (shared across machines) Installation # Homebrew (recommended) brew tap steveyegge/beads brew install bd # Or via go install go install github.com/steveyegge/beads/cmd/bd@latest # Initialize in project cd your-project bd init Quick Reference # Issue management bd create &quot;Task title&quot; -p 1 -t task bd list bd show &lt;id&gt; bd update &lt;id&gt; --status in_progress bd close &lt;id&gt; --reason &quot;Done&quot; # Workflow bd ready # Show unblocked issues bd blocked # Show blocked issues bd stats # View statistics # Dependencies bd dep add &lt;id&gt; &lt;depends-on-id&gt; bd dep tree &lt;id&gt; # Sync bd sync # Force sync now Issue ID Format Hash-based IDs prevent collisions when multiple agents work concurrently: Format: bd-a1b2 , bd-f14c (4-6 char hash) Hierarchical: bd-a3f8e9.1 , bd-a3f8e9.2 for subtasks Daemon Architecture Each workspace runs its own background daemon: Auto-starts on first command Batches operations before export Socket at .beads/bd.sock Disable with BEADS_NO_DAEMON=1 or --no-daemon bd daemons list # Show running daemons bd --no-daemon ready # Skip daemon (for git worktrees) Multi-Agent Workflows (Agent Mail) For 2+ AI agents working concurrently, Agent Mail provides real-time coordination: # Start server git clone https://github.com/Dicklesworthstone/mcp_agent_mail.git cd mcp_agent_mail &amp;&amp; python -m mcp_agent_mail.cli serve-http # Configure each agent export BEADS_AGENT_MAIL_URL=http://127.0.0.1:8765 export BEADS_AGENT_NAME=assistant-alpha export BEADS_PROJECT_ID=my-project Benefits: 20-50x latency reduction (&lt;100ms vs 2-5s git sync) File reservations prevent collision Agents can't claim same issue Data Types Type Description Issue Work item with status, priority, type Dependency Relationship (blocks, related, parent-child) Label Tags with color Comment Threaded discussions Event Audit trail Dependency Types Type Affects bd ready ? blocks Yes - X must close before Y starts parent-child Yes - children blocked if parent blocked related No - soft link for reference discovered-from No - found during work on parent IDE Integration Claude Code bd setup claude # Installs SessionStart/PreCompact hooks Cursor bd setup cursor # Creates .cursor/rules/beads.mdc Context injection uses bd prime which provides ~1-2k tokens of workflow context. Key Files Path Purpose .beads/beads.db SQLite database (gitignored) .beads/issues.jsonl JSONL source of truth (git-tracked) .beads/bd.sock Daemon socket (gitignored) .beads/config.yaml Project config Related Entries Voidlabs Devtools"
    },
    {
      "id": "tooling/voidlabs-devtools",
      "title": "Voidlabs Devtools",
      "tags": "tooling devcontainer developer-experience project",
      "content": "Voidlabs Devtools Shared devcontainer tooling, Claude Code configuration, and project scaffolding for voidlabs projects. Repository Location: /srv/fast/code/voidlabs-devtools GitHub: git@github.com:chriskd/voidlabs-devtools.git Purpose Central tooling repository that provides: Devcontainer templates and scaffolding scripts AI coding agent configuration (Claude Code, Cursor, Factory Droid) Shared shell integration and aliases Beads issue tracker installation and web UI GitHub Actions self-hosted runner setup Architecture devbox.voidlabs.local (Host) ├── /srv/fast/claude-linux/ → Shared Claude config (~/.claude mount) ├── /srv/fast/codex/ → Shared Codex config └── /srv/fast/code/ ├── voidlabs-devtools/ → THIS REPO ├── dotfiles/ → Chezmoi dotfiles └── project-a/, project-b/ → Your projects... Devcontainer mounts: - /srv/fast ← bind mount (same path) - ~/.claude ← bind from /srv/fast/claude-linux - ~/.codex ← bind from /srv/fast/codex - /workspaces/... ← your project Key Scripts new-project.sh Scaffolds new devcontainer projects by fetching templates from GitHub using gh CLI. ./scripts/new-project.sh &quot;My Project&quot; /path/to/my-project ./scripts/new-project.sh --defaults &quot;My Project&quot; /path/to/my-project Creates: AGENTS.md - AI agent guidance .devcontainer/devcontainer.json - Container configuration .devcontainer/Dockerfile - Base image .devcontainer/voidlabs.conf - Feature toggles .devcontainer/scripts/post-start-*.sh - Setup scripts install-beads.sh Installs the beads issue tracker with full integration. /srv/fast/code/voidlabs-devtools/scripts/install-beads.sh /workspaces/my-project post-start-common.sh Runs on devcontainer start, handling: Phase secrets injection Beads installation Factory droid setup Chezmoi dotfiles Worktrunk git worktree manager Project-specific setup Shell Aliases When in a devcontainer: Alias Command Description br bd ready Show tasks ready to work bl bd list List all issues bs bd show Show issue details bst bd stats Project statistics bweb beads-webui.sh Start web UI cdf cd /srv/fast/code Quick navigation Beads Web UI Real-time web dashboard for viewing beads issues: # Build binary (one-time on devbox.voidlabs.local) cd /srv/fast/code/voidlabs-devtools docker build -t beads-webui-builder services/beads-webui docker create --name tmp-beads beads-webui-builder docker cp tmp-beads:/usr/local/bin/beads-webui bin/beads-webui docker rm tmp-beads # Start from any devcontainer bweb # Opens on port 8080 GitHub Actions Self-Hosted Runner Containerized runner for CI at services/gh-runner/ : cd /srv/fast/code/voidlabs-devtools/services/gh-runner cp .env.example .env # Edit .env for GH_PAT, PHASE_APP, etc. docker compose up -d Fetches GH_PAT from Phase at startup - no secrets on disk. Directory Structure voidlabs-devtools/ ├── devcontainers/template/ # New project template ├── ai-tooling/ # AI agent configs │ ├── claude/ # Claude Code settings │ ├── cursor/ # Cursor integration │ ├── factory/ # Factory/Droid config │ └── hooks/ # Claude Code hooks ├── services/ │ ├── gh-runner/ # GitHub Actions runner │ └── beads-webui/ # Beads dashboard ├── scripts/ # Setup utilities ├── shell/devcontainer.zsh # Shell integration └── bin/beads-webui # Pre-built binary Related Entries Voidlabs Infrastructure Overview Beads Issue Tracker"
    },
    {
      "id": "tooling/vl-mail-agent-messaging",
      "title": "vl-mail Agent Messaging",
      "tags": "tooling ai-agents messaging beads cross-project voidlabs-devtools",
      "content": "vl-mail Agent Messaging Lightweight CLI for agent-to-agent messaging with cross-project routing and provenance tracking. Overview vl-mail provides asynchronous message passing between AI agents working on the same or different projects. Messages are stored in JSONL format and synced via git, enabling persistent communication across sessions. Key Features: Cross-project addressing ( user@project routing) Breadcrumb metadata for provenance tracking bd mail integration (via BEADS_MAIL_DELEGATE) JSONL storage ( .vl-mail/messages.jsonl ) Installation Auto-installed in voidlabs devcontainers. For manual installation: cd /srv/fast/code/voidlabs-devtools make install-vl-mail Quick Reference Command Purpose vl-mail inbox Show unread messages vl-mail inbox --all Include read messages vl-mail send &lt;to&gt; -s &quot;Subj&quot; -m &quot;Body&quot; Send message vl-mail read &lt;id&gt; Display message vl-mail ack &lt;id&gt; Mark as read vl-mail reply &lt;id&gt; -m &quot;Body&quot; Reply to message vl-mail sync Git sync mail changes vl-mail help &lt;cmd&gt; Per-command help Cross-Project Addressing # Local delivery (same project) vl-mail send worker-2 -s &quot;Subject&quot; -m &quot;Body&quot; # Cross-project (routes to /srv/fast/code/epstein/.vl-mail/) vl-mail send worker-2@epstein -s &quot;Schema ready&quot; -m &quot;Migrations done&quot; # Global mailbox (routes to ~/.vl-mail/) vl-mail send worker-2@global -s &quot;Announcement&quot; -m &quot;System update&quot; Replies auto-route to the sender's project based on the From field. Breadcrumb Metadata Every message includes provenance metadata captured automatically: Field Source actor BD_ACTOR or USER env var project Git repository name git_branch Current branch git_commit HEAD commit SHA session_id CLAUDE_SESSION_ID beads_issue BD_ISSUE timestamp Message creation time View with vl-mail read msg-xxx --json . Typical Workflow # 1. Check for messages at session start vl-mail inbox # 2. Read and acknowledge vl-mail read msg-abc123 vl-mail ack msg-abc123 # 3. Do the work... # 4. Reply with status vl-mail reply msg-abc123 -m &quot;Completed. Tests passing.&quot; # 5. Sync at session end vl-mail sync Environment Variables Variable Purpose BD_ACTOR / USER Your identity VL_MAIL_BASE Project base path (default: /srv/fast/code ) BEADS_MAIL_DELEGATE Set to &quot;vl-mail&quot; for bd mail passthrough bd mail Integration When BEADS_MAIL_DELEGATE=vl-mail is set (auto-configured in voidlabs devcontainers): bd mail inbox # Passes through to vl-mail bd mail send worker-2 -s &quot;Subject&quot; -m &quot;Body&quot; Storage .vl-mail/ in git repo root (preferred, syncs with repo) ~/.vl-mail/ global fallback (for @global or non-git contexts) Messages stored in messages.jsonl as newline-delimited JSON Source Code Located at /srv/fast/code/voidlabs-devtools/cmd/vl-mail/main.go Uses shared breadcrumb package at pkg/breadcrumb/breadcrumb.go"
    },
    {
      "id": "tooling/vl-mail-lightweight-agent-mail-cli",
      "title": "vl-mail: Lightweight Agent Mail CLI",
      "tags": "tooling agent-mail beads go cli multi-agent",
      "content": "vl-mail: Lightweight Agent Mail CLI A small, single-binary Go CLI for agent-to-agent messaging. Works as bd mail delegate or standalone. Status In Development - Tracked in beads epic voidlabs-devtools-6m5 Why vl-mail? Beads v0.32.0 removed the built-in bd mail commands, delegating mail to external providers like Gas Town ( gt ). However, Gas Town isn't publicly released yet. vl-mail fills this gap with a lightweight alternative. Features 6 commands : inbox , send , read , ack , reply , sync Auto-detect storage : .vl-mail/ in repo (git-synced) or ~/.vl-mail/ global Zero deps : Go stdlib only, ~400-500 lines bd integration : Works as bd mail delegate Interface Command Args Description inbox [--json] List unread messages for current identity send &lt;to&gt; -s &quot;Subj&quot; -m &quot;Body&quot; [--urgent] Send message read &lt;id&gt; Show message details ack &lt;id&gt; Mark as read reply &lt;id&gt; -m &quot;Body&quot; Reply to message sync Git pull/push .vl-mail/ Data Model Messages stored as JSONL: {&quot;id&quot;:&quot;msg-a1b2&quot;,&quot;to&quot;:&quot;worker-2&quot;,&quot;from&quot;:&quot;claude-main&quot;,&quot;subject&quot;:&quot;Handoff&quot;,&quot;body&quot;:&quot;Your turn&quot;,&quot;status&quot;:&quot;unread&quot;,&quot;priority&quot;:2,&quot;reply_to&quot;:&quot;&quot;,&quot;created&quot;:&quot;2025-12-22T10:00:00Z&quot;} Setup As bd mail delegate bd config set mail.delegate &quot;/srv/fast/code/voidlabs-devtools/bin/vl-mail&quot; # Then use: bd mail inbox, bd mail send, etc. Standalone vl-mail inbox vl-mail send worker-2 -s &quot;Handoff&quot; -m &quot;Your turn on bd-xyz&quot; Location Source: voidlabs-devtools/cmd/vl-mail/ Binary: voidlabs-devtools/bin/vl-mail Related beads-workflow - Beads issue tracking integration voidlabs-devtools - Parent tooling repository"
    },
    {
      "id": "tooling/project-template-sync-pbs",
      "title": "Project Template Sync (pbs)",
      "tags": "tooling devcontainer workflow",
      "content": "Project Template Sync (pbs) pbs (Project Bootstrap and Sync) keeps devcontainer configurations in sync across all voidlabs projects. Concept The template at voidlabs-devtools/devcontainers/template/.devcontainer/ is the source of truth . Projects should mirror this template, with pbs managing the sync. Commands Command Description pbs status Show drift from template (current project) pbs status --all Show drift across all projects pbs diff Show actual file diffs pbs diff &lt;file&gt; Show diff for specific file pbs sync Apply template to current project pbs sync --all Apply template to all projects pbs edit &lt;file&gt; Edit template file, then apply to all pbs files List which files are synced vs excluded pbs new &lt;path&gt; Bootstrap a new devcontainer project Synced Files These files are managed by pbs (edit in template, not in projects): devcontainer.json docker-compose.yml scripts/post-start-common.sh configs/worktrunk-user-config.toml Project-Specific Files These files are NOT synced (edit in each project): Dockerfile - Project-specific build post-start-project.sh - Project-specific setup voidlabs.conf - Project feature flags docker-compose.override.yml - Project overrides Workflow for Agents Improving shared setup : Edit template in devcontainers/template/ , then pbs sync --all Project-specific changes : Edit directly in project's .devcontainer/ Check sync status : Run pbs status to see if project has drifted Environment Variables PBS_TEMPLATE_DIR - Path to template .devcontainer/ directory PBS_PROJECTS_ROOT - Path to scan for projects (for --all)"
    },
    {
      "id": "projects/dictaphone-inbox-pattern-feature-plan",
      "title": "Dictaphone Inbox Pattern Feature Plan",
      "tags": "dictaphone feature plan inbox auto-organize",
      "content": "Auto-Organize Typed Text (Inbox Pattern) Beads Issue: dictaphone-lm3 Status: Planned Summary Extend the Dictaphone notepad to auto-organize typed text (not just voice transcriptions) using an LLM-driven &quot;inbox pattern&quot;. Use Case Breaking news reporter scenario - capture information fast without stopping to organize. Drop text into an inbox heading, it auto-relocates to the appropriate category, keep moving. Design Decisions Decision Choice Item format Standard markdown lists ( - , * , 1. , etc.) Trigger Both timeout + hotkey, user configurable Confidence 3 modes: auto-all, auto-high/flag-uncertain, keep-uncertain Learning None initially (defer to future iteration) Architecture Voice path (existing): InlineRecordingBar → TranscriptionService → NotepadView.processTranscription() → CategorizationService.categorize() → ContentSyncService.mergeItems() Typed text path (new): NotepadView (inbox monitoring) → InboxProcessor (new) → CategorizationService.categorize() ← reuse → DocumentParser.moveItem() ← reuse Implementation Phases Phase 1 (dictaphone-t2x): InboxSettings model Phase 2 (dictaphone-5ao): Inbox detection in DocumentParser Phase 3 (dictaphone-uxq): InboxProcessor service Phase 4 (dictaphone-m1a): Example context building Phase 5 (dictaphone-kzd): NotepadView integration Phase 6 (dictaphone-t2d): Visual feedback components Phase 7 (dictaphone-26e): InboxSettingsView Key Files File Action Purpose InboxSettings.swift Create Settings model InboxProcessor.swift Create Core processing logic OrganizedItemToast.swift Create Visual feedback InboxSettingsView.swift Create Settings UI DocumentParser.swift Modify Section/item helpers ContentSyncService.swift Modify Track processed items CategorizationService.swift Modify Example context NotepadView.swift Modify Integration point Future Enhancements (Not in Scope) Correction memory / learning from feedback Drag-drop item relocation Suggested new categories Batch processing UI for uncertain items relates_to projects/dictaphone-app-ios-and-macos-voice-categorization-app.md relates_to projects/dictaphone-implementation-progress.md"
    },
    {
      "id": "projects/test-beads-full-project",
      "title": "Test Beads Integration - Full Project",
      "tags": "test beads",
      "content": "Test Beads Integration - Full Project This is a test entry to verify beads integration with a full project link. Expected Behavior When viewing this entry in the webapp, the sidebar should show: All open issues from the memex project Issues should be displayed with status dots and priority labels Frontmatter Config beads_project: /srv/fast/code/memex Note: No beads_issues field - this means show all open project issues."
    },
    {
      "id": "projects/dictaphone-phase-7-8-implementation",
      "title": "Dictaphone Phase 7-8 Implementation",
      "tags": "dictaphone swift ios macos phase7 phase8",
      "content": "Dictaphone Phase 7-8 Implementation Phase 7: Note Hierarchy &amp; Management Key Components HierarchicalNoteRow : Recursive SwiftUI component with expand/collapse chevrons Expansion State : Set&lt;UUID&gt; for O(1) lookups tracking which notes are expanded Context Menu : Full CRUD operations (create sub-note, rename, delete, category defaults) NoteCategoryDefaultsView : Sheet with inheritance toggle for child notes Design Decisions Used VStack with manual recursion instead of OutlineGroup for more control Indentation via padding(.leading, CGFloat(depth) * 20) Search recursively matches both parent and child titles Phase 8: Export &amp; Platform Integration ExportService Three formats: Text (human-readable), CSV (spreadsheet), JSON (programmatic) ExportOptions struct controls metadata, confidence, children inclusion CSV escaping handles commas, quotes, and newlines properly Platform Integration iOS AppShortcutsProvider exposes shortcuts to Siri, Shortcuts app, Action Button AppIntent with perform() async function for Start/Stop Recording, Create/Export Note AppEntity + EntityQuery for referencing notes and formats macOS Commands protocol for menu items via scene modifier AppState (@Observable) bridges menu actions to views Keyboard shortcuts: ⌘⇧R (record), ⌘. (stop), ⌘1/2/, (navigation) Share Sheet iOS: UIActivityViewController with temporary file URL macOS: NSSavePanel or clipboard copy Swift 6 Concurrency Notes static let required for App Intent properties (immutable global state) @Observable for cross-view state management #if os(iOS/macOS) for platform-specific code"
    },
    {
      "id": "projects/tg-scrape-social-media-astroturfing-analysis-project",
      "title": "TG Scrape: Social Media Astroturfing Analysis Project",
      "tags": "tg-scrape apify facebook astroturfing sqlite social-media-analysis",
      "content": "TG Scrape Project Documentation Overview Investigative journalism project to analyze potential astroturfing in social media responses to political posts. Source Data PDF : &quot;TG monitoring binder.pdf&quot; (810 pages, ~10MB) Printed emails containing social media links Links were flagged by a politician's team for monitoring Extracted Links (stored in tg_scrape.db) Total unique links : 11,717 By platform: Twitter: 5,170 (mostly tweets) YouTube: 3,184 Facebook: 1,862 (1,186 posts) Reddit: 1,409 Instagram: 71 TikTok: 21 Database Schema Located at /srv/fast/code/tg-scrape/schema.sql Key tables: links - Source URLs from PDF posts - Scraped post content replies - Comments/replies on posts authors - Unique commenters for tracking repeat actors scrape_jobs - Track Apify job status Apify Actors Used Platform Actor Cost Facebook comments apify/facebook-comments-scraper $0.006/start + $0.0025/comment Twitter replies scraper_one/x-post-replies-scraper $0.0025/start + $0.00025/reply Scraping Strategy Budget : $40 Apify credits Chosen approach : Broad coverage - all 1,186 FB posts, 10 comments each (~$30) Posts are batched (50 per run) to minimize start costs Key Files extract_links.py - PDF to SQLite link extraction schema.sql - Database schema db.py - Database utilities scraper.py - Apify integration and cost estimation store_results.py - Store scraped data in SQLite Analysis Goals Find repeat actors (same profileId across multiple posts) Timing pattern analysis (coordinated responses) Content similarity detection Network analysis of reply patterns Status [x] PDF link extraction complete [x] SQLite schema created [x] Apify scraping tested and working [ ] Full Facebook scrape (1,186 posts) [ ] Twitter/X scraping [ ] Analysis queries Commands # Check costs python3 scraper.py estimate # View database stats python3 scraper.py stats # Extract links from PDF python3 extract_links.py"
    },
    {
      "id": "projects/concordance-datopt-import-patterns",
      "title": "Concordance DAT/OPT Import Patterns",
      "tags": "docviewer concordance legal-discovery data-import dat-opt s3",
      "content": "Concordance DAT/OPT Import Patterns Legal discovery productions often use the Concordance format. Here's what we learned importing DOJ EFTA datasets. File Format Details DAT Files (Document Metadata) Encoding : UTF-8 with BOM ( utf-8-sig ) Delimiter : \\x14 (DC4 control character) Quote Character : þ (thorn, 0xFE) Line Endings : CRLF (Windows) OPT Files (Image-to-Page Mappings) Standard CSV format Maps bates numbers to image file paths Columns: bates_number, volume, path, is_first_page, unused, unused, pages_count Common Header Variations Different productions use different header names: Expected Variation Found Bates Begin Begin Bates Bates End End Bates Fix before import: sed '1s/Begin Bates/Bates Begin/; 1s/End Bates/Bates End/' input.DAT &gt; fixed.DAT Path Transformation for S3 OPT files contain Windows paths like IMAGES\\0001\\EFTA00000001.pdf . For S3 storage, transform to: s3://bucket/dataset-slug/0001/EFTA00000001.pdf Python snippet: raw_path = row[2] # IMAGES\\0001\\file.pdf normalized = raw_path.replace('\\\\', '/').replace('IMAGES/', '') s3_path = f&quot;s3://efta-images/{slug}/{normalized}&quot; Import Workflow Fix headers if needed (sed) Import DAT with --skip-opt (document metadata) Transform OPT paths to S3 URIs Insert image records directly to DB Related Issues epstein-s5g - Header variation support epstein-559 - S3 path transformation feature"
    },
    {
      "id": "projects/dictaphone-phase-5-core-ui-implementation",
      "title": "Dictaphone Phase 5 - Core UI Implementation",
      "tags": "projects dictaphone swiftui ui",
      "content": "Dictaphone Phase 5 - Core UI Implementation Completed: 2026-01-05 RecordingView (&quot;Warm Studio&quot; Design) Aesthetic choices: Dark gradient background with warm undertones Amber accent color (#FFC040) reminiscent of vintage VU meters Serif font for transcription creates editorial readability Glassmorphism for panels and chips Key features: Live waveform visualization (WaveformView component) Real-time transcription with partial text in amber Recording button with breathing pulse animation Categorized items appear as floating amber-tinted chips Timer with contentTransition(.numericText()) for smooth digits NoteDetailView Architecture: Collapsible category sections (CategorySectionView) Native drag-drop using draggable() and dropDestination() Confidence-based color coding: Green: 90%+ Yellow: 70-90% Orange: 50-70% Red: &lt;50% Interactions: Swipe actions (left: move, right: delete) Context menu for edit/move/delete Uncertain items banner with tap-to-review Uncertain Items Review Card-based swipe UI: Progress bar showing review completion Confirm (marks confidence = 1.0) Move to different category Delete item Spring animations between cards Platform-Adaptive Navigation iOS: TabView with Notes, Record, Settings tabs .tint(.amber) for consistent accent NavigationStack per tab with .navigationDestination macOS: NavigationSplitView with sidebar Three-column layout: sidebar, note list, detail .navigationSplitViewStyle(.balanced) Files Created/Modified RecordingView.swift - Full-featured recording interface NoteDetailView.swift - Category sections with drag-drop NoteListView.swift - Updated with navigation + selection ContentView.swift - Platform-specific navigation ColorExtensions.swift - Shared .amber color Key Patterns Category colors generated from name hash for consistency @Bindable for two-way SwiftData binding .sensoryFeedback() for haptic feedback on record ScrollViewReader for auto-scroll to transcription bottom"
    },
    {
      "id": "projects/cli-tool-path-lookup-behavior",
      "title": "CLI Tool PATH Lookup Behavior",
      "tags": "focusgroup configuration troubleshooting",
      "content": "CLI Tool PATH Lookup Behavior Focusgroup invokes external CLI tools ( claude , codex ) via subprocess and relies on standard PATH resolution to locate them. How It Works When focusgroup needs to run an agent CLI: PATH lookup : Uses Python's shutil.which() to verify the command exists Subprocess execution : Runs the command directly via asyncio.create_subprocess_exec() Error on failure : Raises a clear error if the command is not found Error Messages If a CLI tool is not in PATH, you'll see errors like: Claude CLI not found. Is it installed and in PATH? Codex CLI not found. Is it installed and in PATH? Solutions Option 1: Ensure CLIs are in PATH Add the directory containing your CLI tools to your PATH environment variable: # Add to ~/.bashrc or ~/.zshrc export PATH=&quot;$HOME/.local/bin:$PATH&quot; # Verify it works which claude which codex Option 2: Use absolute paths in configuration If your CLIs are installed in non-standard locations, you can specify absolute paths in your focusgroup configuration file: [agents.claude] command = &quot;/opt/anthropic/bin/claude&quot; [agents.codex] command = &quot;/usr/local/bin/codex&quot; Technical Details The PATH lookup is implemented in two places: CLITool class ( src/focusgroup/tools/cli.py ): General CLI wrapper using shutil.which() Agent implementations ( src/focusgroup/agents/claude.py , codex.py ): Direct subprocess calls Both raise descriptive errors when commands are not found, helping users diagnose PATH issues quickly. Related Focusgroup Project - Project overview"
    },
    {
      "id": "projects/docviewer-naming-conventions",
      "title": "DocViewer Naming Conventions",
      "tags": "docviewer naming conventions documentation",
      "content": "DocViewer Naming Conventions Project Directory The project currently lives at /srv/fast/code/epstein , but the canonical name is DocViewer . The &quot;epstein&quot; name is a historical artifact (named after the Epstein documents dataset). Future plan : Rename directory to docviewer for consistency. Compose Service Names Production ( docker-compose.prod.yml ) Service Purpose db ParadeDB database backend FastAPI application frontend Nginx + React SPA migrations One-shot migration runner Dokploy Generated Names Dokploy adds random suffixes to avoid conflicts: Compose: compose-bypass-open-source-pixel-szoiw5 Services: docviewer-epsteindb-uouojr Use partial matching in CLI: dokploy compose info docviewer Database Naming Environment Database Host:Port Production epstein (internal) db:5432 Development epstein dokploy.voidlabs.cc:5433 Tables Table Purpose documents Document metadata document_embeddings Vector embeddings per chunk images OCR page images tags Document tags datasets Evaluation dataset metadata API Paths Path Purpose /api/* All API endpoints /health Health check (not under /api) /docs OpenAPI documentation Environment Variables Database DATABASE_URL=postgresql+psycopg://user:pass@host:port/dbname POSTGRES_USER=epstein POSTGRES_PASSWORD=&lt;secret&gt; POSTGRES_DB=epstein Embedding EMBEDDING_PROVIDER=voyage|openai|text_embeddings_inference EMBEDDING_MODEL=voyage-context-3 EMBEDDING_DIMENSIONS=1024 Vector Store QDRANT_URL=http://host:6333 QDRANT_COLLECTION=epstein-document-embeddings File Naming Compose Files File Purpose docker-compose.prod.yml Production (Dokploy) ~~ docker-compose.yml ~~ Removed - use scripts/dev.sh ~~ dokploy-stack.yml ~~ Removed - replaced by prod.yml Scripts File Purpose scripts/dev.sh Local development startup Related docviewer-deployment-architecture Voidlabs Common Patterns"
    },
    {
      "id": "projects/dictaphone-phase-6-cloud-providers-implementation",
      "title": "Dictaphone Phase 6 - Cloud Providers Implementation",
      "tags": "dictaphone phase-6 ios macos swift cloud openai anthropic",
      "content": "Dictaphone Phase 6 - Cloud Providers Implementation Implementation of cloud-based transcription and categorization providers for the Dictaphone app. Components Implemented 1. APIKeyManager (Security/APIKeyManager.swift) Secure Keychain storage for API keys using Security framework: Actor-based for thread safety APIKeyType enum : .openAI , .anthropic Operations : saveKey() , getKey() , deleteKey() , hasKey() , validateKeyFormat() Security : Uses kSecAttrAccessibleAfterFirstUnlock for background access 2. WhisperCloudProvider (Transcription/WhisperCloudProvider.swift) OpenAI Whisper API integration for cloud transcription: Converts Float32 audio to WAV format Chunked processing (~10 second chunks) for pseudo-streaming Context preservation via prompt parameter Multipart form-data upload 3. OpenAIProvider (Categorization/OpenAIProvider.swift) GPT-4 Turbo for cloud categorization: JSON mode ( response_format: { type: &quot;json_object&quot; } ) Structured prompts for item extraction Temperature 0.3 for consistency 4. AnthropicProvider (Categorization/AnthropicProvider.swift) Claude 3.5 Sonnet for cloud categorization: Uses Messages API with x-api-key header JSON extraction from text responses Same structured output format as OpenAI 5. SettingsView Updates API key management UI with save/update/remove buttons, key validation, and status indicators. Platform Compatibility Fixes Added platform-adaptive system colors to ColorExtensions.swift and fixed iOS-only modifiers. Related projects/dictaphone-implementation-progress.md projects/dictaphone-phase-5-core-ui-implementation.md"
    },
    {
      "id": "projects/docviewer-deployment-architecture",
      "title": "DocViewer Deployment Architecture",
      "tags": "docviewer deployment docker dokploy architecture",
      "content": "DocViewer Deployment Architecture Overview DocViewer uses two distinct deployment modes: Mode Tool Database When to Use Production docker-compose.prod.yml via Dokploy Internal ParadeDB Real deployments Local Dev Native processes Dev DB on Dokploy Active development Production Architecture ┌─────────────────────────────────────────────────────┐ │ dokploy.voidlabs.cc │ │ │ │ ┌─────────────────────────────────────────────┐ │ │ │ docviewer-prod compose │ │ │ │ │ │ │ │ frontend:80 ──▶ backend:8000 ──▶ db:5432 │ │ │ │ (nginx+SPA) (FastAPI) (ParadeDB) │ │ │ │ │ │ │ │ All internal except frontend │ │ │ └─────────────────────────────────────────────┘ │ └─────────────────────────────────────────────────────┘ Key Points Git-based deployment : Push to main triggers deploy via Dokploy webhook Internal networking : All services internal, only frontend exposes port 80 API proxy : Frontend nginx proxies /api/* to backend Database : ParadeDB (Postgres + pgvector + BM25) runs inside compose Migrations : One-shot migration service runs on deploy Files File Purpose docker-compose.prod.yml Production compose stack frontend/nginx.conf Nginx config with API proxy Dockerfile Backend build frontend/Dockerfile Frontend build (nginx + SPA) Deployment Commands # View status dokploy compose info docviewer-prod # Redeploy (pulls from git) dokploy compose deploy docviewer-prod # View logs dokploy compose logs docviewer-prod Development Architecture ┌───────────────────────────────────────────────────────────────┐ │ devbox.voidlabs.local (devcontainer) │ │ │ │ ┌─────────────────────────────────────────────────────────┐ │ │ │ Native processes via scripts/dev.sh │ │ │ │ │ │ │ │ Frontend (Vite) Backend (uvicorn) │ │ │ │ localhost:5173 localhost:8000 │ │ │ └─────────────────────────────────────────────────────────┘ │ │ │ │ │ ▼ │ │ dokploy.voidlabs.cc:5433 (postgres-dev) │ └───────────────────────────────────────────────────────────────┘ Why Native Processes? Faster iteration : No container rebuild on code changes Shared database : Uses postgres-dev on Dokploy (port 5433) Hot reload : Both Vite and uvicorn have hot reload Development Commands # Start everything ./scripts/dev.sh # Start only frontend ./scripts/dev.sh frontend # Start only backend ./scripts/dev.sh backend # Run migrations only ./scripts/dev.sh migrate Troubleshooting Migration failures on deploy If migrations fail with column/table errors, the database may be in a corrupt state from a previous failed deploy. Clean up: ssh chris@dokploy.voidlabs.cc docker stop &lt;db-container&gt; docker rm &lt;db-container&gt; docker volume rm &lt;pgdata-volume&gt; dokploy compose deploy docviewer-prod Service naming in Dokploy Dokploy generates unique service names with random suffixes. Use docker service ls to find actual names. Related dokploy-deployment-guide Voidlabs Common Patterns ## Production Architecture Production Architecture ┌─────────────────────────────────────────────────────┐ │ dokploy.voidlabs.cc │ │ │ │ ┌─────────────────────────────────────────────┐ │ │ │ docviewer-prod compose │ │ │ │ │ │ │ │ frontend:80 ──▶ backend:8000 ──▶ db:5432 │ │ │ │ (nginx+SPA) (FastAPI) (ParadeDB) │ │ │ │ │ │ │ │ │ └──────────▶ Garage S3 │ │ │ │ (efta-images) │ │ │ └─────────────────────────────────────────────┘ │ └─────────────────────────────────────────────────────┘ Key Points Git-based deployment : Push to main triggers deploy via Dokploy webhook Internal networking : All services internal, only frontend exposes port 80 API proxy : Frontend nginx proxies /api/* to backend Database : ParadeDB (Postgres + pgvector + BM25) runs inside compose Migrations : One-shot migration service runs on deploy Image storage : S3-compatible storage via Garage ( efta-images bucket) S3 Storage Configuration Production uses Garage S3 for document images: Variable Value STORAGE_DRIVER s3 STORAGE_S3_ENDPOINT https://garage-fast.voidlabs.cc STORAGE_S3_BUCKET efta-images STORAGE_S3_REGION garage Access keys are configured in Dokploy's environment variables (not in git). See garage-s3-object-storage for Garage administration."
    },
    {
      "id": "projects/focusgroup-project",
      "title": "Focusgroup Project",
      "tags": "projects focusgroup agent-tooling",
      "content": "Focusgroup A tool for gathering feedback from multiple LLM agents on tools being designed with agents as the primary users. Purpose When building CLI tools that will primarily be consumed by AI agents (like memex), use focusgroup to consult multiple agents about: Features and capabilities Design choices and API ergonomics Interactive tests and workflows UX improvements from the agent perspective Key Insight Agents are both the operators of focusgroup (running the consultations) and the subjects being consulted. This meta-quality means the tool must be exceptionally agent-friendly in its own interface. Primary Use Case Testing and improving memex by having multiple different LLM agents provide feedback on: Search interface design Entry format and structure CLI ergonomics for agent use What information agents wish they could find Status Greenfield project as of 2026-01-04. No code yet. Location /srv/fast/code/focusgroup"
    },
    {
      "id": "projects/dictaphone-app-ios-and-macos-voice-categorization-app",
      "title": "Dictaphone App - iOS and macOS Voice Categorization App",
      "tags": "projects ios macos swift swiftui dictaphone ai multiplatform",
      "content": "Dictaphone App - Implementation Plan Overview A native iOS 18+ and macOS 15+ dictaphone app for organizing items during a move. Speak naturally (&quot;harmony remote...goes under give away&quot;) and AI transcribes + categorizes items into lists. Requirements Summary Core Features (MVP) Continuous listening while app is open (primary mode) Push-to-talk via iOS Action Button / macOS keyboard shortcut (secondary mode) On-device transcription via Apple Speech framework AI categorization via Apple Intelligence Foundation Models Hybrid categories : User-defined defaults + AI-created dynamic categories Hierarchical notes : Notes &gt; Sub-notes &gt; Categorized Items Confidence flagging : Uncertain items marked for review Corrections : Both voice (&quot;move X to Y&quot;) and manual drag-drop Audio retention : User choice (discard/keep session/keep permanent) Export : Text, CSV, JSON file export Cloud Provider Support Users can mix-and-match local vs cloud for each AI function: Function Local Option Cloud Options Transcription Apple SpeechAnalyzer OpenAI Whisper, Deepgram, AssemblyAI Categorization Apple Intelligence OpenAI GPT-4, Anthropic Claude This creates 4 valid configurations: Full local (privacy, offline, free) Full cloud (max accuracy, works on older devices) Local transcription + cloud categorization Cloud transcription + local categorization Technical Architecture Platform &amp; Target iOS 18+ and macOS 15+ native (Swift/SwiftUI) Xcode 16+ SwiftData for persistence (synced via iCloud) MVVM with @Observable pattern Multiplatform target (shared codebase, platform-specific adaptations) Multiplatform Considerations Feature iOS macOS Push-to-talk Action Button / Control Center Global keyboard shortcut (⌘⇧R) Recording UI Full-screen / sheet Floating panel or menu bar app Navigation TabView Sidebar NavigationSplitView Audio session AVAudioSession AVAudioEngine (no session needed) Permissions Info.plist keys Info.plist keys + sandbox entitlements Background audio UIBackgroundModes NSSupportsAutomaticTermination = NO Project Structure Dictaphone/ ├── DictaphoneApp.swift ├── ContentView.swift │ ├── Core/ │ ├── Models/ # SwiftData models (shared) │ │ ├── Note.swift │ │ ├── Category.swift │ │ ├── Item.swift │ │ └── AppSettings.swift │ │ │ ├── Services/ # Shared services │ │ ├── Audio/ │ │ ├── Transcription/ │ │ ├── Categorization/ │ │ └── Export/ │ │ │ ├── Repositories/ │ └── Utilities/ │ ├── Features/ │ ├── NoteList/ │ ├── Recording/ │ ├── NoteDetail/ │ └── Settings/ │ ├── Platform/ # Platform-specific code │ ├── iOS/ │ │ ├── iOSContentView.swift │ │ └── ActionButtonIntent.swift │ └── macOS/ │ ├── macOSContentView.swift │ ├── MenuBarView.swift │ └── GlobalShortcuts.swift │ ├── AppIntents/ └── Resources/ ├── Assets.xcassets/ ├── Info.plist (iOS) └── Info.plist (macOS) Data Models Note (SwiftData) @Model class Note { var id: UUID var title: String var createdAt: Date var parent: Note? // Hierarchy support var children: [Note] var categories: [Category] var defaultCategoryNames: [String] // User-defined defaults var audioRetentionPolicy: AudioRetentionPolicy } Category (SwiftData) @Model class Category { var id: UUID var name: String var isDefault: Bool // User-defined vs AI-created var sortOrder: Int var note: Note var items: [Item] } Item (SwiftData) @Model class Item { var id: UUID var name: String var confidence: Double // 0.0-1.0 var isUncertain: Bool { confidence &lt; 0.7 } var timestamp: Date var audioRelativePath: String? // Optional audio storage var category: Category } AppSettings (SwiftData) @Model class AppSettings { var transcriptionProvider: TranscriptionProviderType var categorizationProvider: CategorizationProviderType var defaultAudioRetention: AudioRetentionPolicy var uncertaintyThreshold: Double // Default 0.7 var autoCreateCategories: Bool // Default true } Provider Architecture TranscriptionProvider Protocol protocol TranscriptionProvider: Actor { var providerType: TranscriptionProviderType { get } var requiresAPIKey: Bool { get } var isOnDevice: Bool { get } var isTranscribing: Bool { get } func checkAvailability() async -&gt; TranscriptionProviderAvailability func requestAuthorization() async -&gt; Bool func startTranscription(audioStream: AsyncStream&lt;SendableAudioData&gt;) async throws -&gt; AsyncStream&lt;TranscriptionResult&gt; func stopTranscription() async } Implementation Note : Provider is actor-isolated for thread-safe state management. Uses SendableAudioData (not raw AVAudioPCMBuffer) to comply with Swift 6 strict concurrency. CategorizationProvider Protocol protocol CategorizationProvider { var isAvailable: Bool { get async } var requiresAPIKey: Bool { get } func categorize(text: String, availableCategories: [String], instructions: String?) async throws -&gt; CategorizationResult } Provider Implementations Provider Type Platforms AppleSpeechProvider Transcription iOS 18+, macOS 15+ WhisperCloudProvider Transcription All AppleIntelligenceProvider Categorization iOS 18+, macOS 15+ (Apple Silicon) OpenAIProvider Categorization All AnthropicProvider Categorization All Implementation Phases Phase 1: Project Setup &amp; Core Models ✅ Xcode project (iOS 18+, SwiftUI) SwiftData models Info.plist permissions Basic navigation shell Phase 2: Audio Recording Infrastructure ✅ AVAudioEngine setup (cross-platform) Platform-specific audio session handling Waveform visualization AsyncStream-based audio data pipeline Phase 3: Local Transcription (Apple Speech) ✅ SFSpeechRecognizer with streaming buffer recognition TranscriptionProvider protocol (actor-based) TranscriptionService facade with @Published state Real-time partial/final results with confidence scores See Implementation Progress for architecture details. Phase 4: Local Categorization (Apple Intelligence) Foundation Models integration Availability checking per platform Phase 5: Core UI - Recording &amp; Note Detail Platform-adaptive layouts Drag-drop reorganization Phase 6: Cloud Providers API key management (Keychain) Whisper, OpenAI, Anthropic providers Phase 7: Note Hierarchy &amp; Management Hierarchical note list Create/delete/rename actions Phase 8: Export &amp; Platform Integration Export service (Text, CSV, JSON) iOS: Action Button via App Intents macOS: Global keyboard shortcuts, menu bar Phase 9: Polish &amp; Settings Comprehensive Settings UI Platform-specific polish Permissions Required iOS (Info.plist) &lt;key&gt;NSMicrophoneUsageDescription&lt;/key&gt; &lt;string&gt;Record voice memos for transcription and categorization&lt;/string&gt; &lt;key&gt;NSSpeechRecognitionUsageDescription&lt;/key&gt; &lt;string&gt;Transcribe your speech to text on-device&lt;/string&gt; &lt;key&gt;UIBackgroundModes&lt;/key&gt; &lt;array&gt;&lt;string&gt;audio&lt;/string&gt;&lt;/array&gt; macOS (Info.plist + Entitlements) &lt;!-- Info.plist --&gt; &lt;key&gt;NSMicrophoneUsageDescription&lt;/key&gt; &lt;string&gt;Record voice memos for transcription and categorization&lt;/string&gt; &lt;key&gt;NSSpeechRecognitionUsageDescription&lt;/key&gt; &lt;string&gt;Transcribe your speech to text on-device&lt;/string&gt; &lt;!-- Entitlements --&gt; &lt;key&gt;com.apple.security.device.audio-input&lt;/key&gt; &lt;true/&gt; Decisions Made Question Decision Platforms iOS 18+ and macOS 15+ (multiplatform target) Bundle ID com.voidlabs.dictaphone Cloud providers OpenAI + Anthropic API key management Keychain storage, user provides keys Sync SwiftData + iCloud (future) Recording format 44.1kHz mono Float32, streamed via AsyncStream Speech recognition Resampled to 16kHz for optimal SFSpeechRecognizer performance Concurrency model Swift 6 strict concurrency with actor-isolated providers Cross-boundary data Extract to Sendable structs before crossing actor boundaries"
    },
    {
      "id": "projects/sightline-5ightline",
      "title": "SightLine (5ightline Project)",
      "tags": "project facial-recognition aws-rekognition fastapi react lambda",
      "content": "SightLine (5ightline Project) Web-based facial recognition investigation tool for investigative journalism. Leverages AWS Rekognition for facial analysis. Repository Location: /srv/fast/code/5ightline Architecture ┌─────────────────────────────────────────────────────────────────┐ │ Frontend (React SPA) │ │ ┌─────────────┐ ┌─────────────┐ ┌───────────────┐ ┌───────────┐ │ │ │ Collections │ │ Faces │ │ Rekognition │ │ Jobs │ │ │ │ Management │ │ Management │ │ Users │ │ Processing│ │ │ └─────────────┘ └─────────────┘ └───────────────┘ └───────────┘ │ └────────────────────────────┬────────────────────────────────────┘ │ HTTPS/REST ┌────────────────────────────┴────────────────────────────────────┐ │ Backend API (FastAPI) │ │ ┌──────────────────────────────────────────────────────────┐ │ │ │ Service Layer │ │ │ │ ┌────────────┐ ┌────────────┐ ┌──────────┐ ┌──────────┐ │ │ │ │ │Rekognition │ │ Image │ │ Job │ │ Health │ │ │ │ │ │ Service │ │ Processor │ │Processor │ │ Check │ │ │ │ │ └────────────┘ └────────────┘ └──────────┘ └──────────┘ │ │ │ └──────────────────────────────────────────────────────────┘ │ └─────────┬──────────────────┬──────────────────┬────────────────┘ │ │ │ ┌─────┴─────┐ ┌──────┴──────┐ ┌─────┴─────┐ │ SQLite │ │ AWS │ │ AWS S3 │ │ Database │ │ Rekognition │ │ Storage │ └───────────┘ └─────────────┘ └───────────┘ Technology Stack Layer Technology Frontend React 18 + Material-UI v5 + React Router v6 Backend FastAPI + SQLAlchemy 2.0 + Pydantic Database SQLite (dev) / PostgreSQL (prod) File Storage AWS S3 Facial Recognition AWS Rekognition Package Management UV (Python), npm (JS) Lambda Functions Located in lambdas/ : Function Purpose job-processor Starts Rekognition StartFaceSearch on S3 videos results-to-ddb Consumes Rekognition SNS messages via SQS extract-frame Extracts video frames for matches Lambda Environment Variables AWS_REGION REK_ROLE_ARN REK_SNS_TOPIC_ARN SUBMIT_QUEUE_URL RESULTS_QUEUE_URL EXTRACT_QUEUE_URL OUTPUT_BUCKET OUTPUT_PREFIX DDB_TABLE_JOBS DDB_TABLE_MATCHES Deploy (SAM) cd lambdas sam build --use-container --parallel sam deploy --guided API Endpoints Collections GET/POST /api/v1/collections GET/PUT/DELETE /api/v1/collections/{id} POST /api/v1/collections/sync-aws Faces GET/POST /api/v1/faces GET/DELETE /api/v1/faces/{id} Rekognition Users GET/POST /api/v1/rekognition-users GET/PUT/DELETE /api/v1/rekognition-users/{id} PUT /api/v1/rekognition-users/{id}/faces Jobs GET/POST /api/v1/jobs GET/DELETE /api/v1/jobs/{id} POST /api/v1/jobs/{id}/cancel GET /api/v1/jobs/{id}/results S3 Bucket Structure rekognition-investigation-images/ ├── collections/{collection_id}/ │ ├── source-images/ │ └── face-crops/ ├── media-sets/{media_set_id}/ │ ├── images/ │ └── thumbnails/ ├── temp/processing/{job_id}/ └── exports/{export_id}/ Development Workflow # Backend cd backend source venv/bin/activate uvicorn app.main:app --reload # Frontend cd frontend npm run dev # Access # Frontend: http://localhost:5173 # Backend API: http://localhost:8000 # API Docs: http://localhost:8000/docs Database Schema (Core Tables) collections - Face collections in AWS Rekognition media_sets - Collections of images/videos to search media_set_files - Individual files within media sets detection_sessions - Temporary face detection for user selection rekognition_users - User profiles for organizing faces faces - Indexed faces in collections jobs - Analysis/search jobs results - Job match results Key Features Two-Step Face Selection: Detection sessions allow users to select which faces to index Job Progress: Server-Sent Events (SSE) for real-time progress tracking Batch Processing: Queue-based architecture for large media sets Related Entries Voidlabs Infrastructure Overview AWS Rekognition Integration"
    },
    {
      "id": "projects/test-beads-single-issue",
      "title": "Test Beads Integration - Single Issue",
      "tags": "test beads",
      "content": "Test Beads Integration - Single Issue This is a test entry to verify beads integration with a single linked issue. Expected Behavior When viewing this entry in the webapp, the sidebar should show: The linked issue voidlabs-devtools-7ai from voidlabs-devtools Issue title: &quot;Add vl-mail prime command for session context&quot; Status: closed Frontmatter Config beads_project: /srv/fast/code/voidlabs-devtools beads_issues: - voidlabs-devtools-7ai"
    },
    {
      "id": "projects/docviewer-local-development-workflow",
      "title": "DocViewer Local Development Workflow",
      "tags": "docviewer development workflow local-dev s3",
      "content": "DocViewer Local Development Workflow Quick Start cd /srv/fast/code/epstein ./scripts/dev.sh This starts: Frontend (Vite): http://localhost:5173 Backend (uvicorn): http://localhost:8000 Database : Uses postgres-dev on dokploy.voidlabs.cc:5433 Storage : Uses S3 (Garage) - same bucket as production Prerequisites Dependency Required For node + npm Frontend (Vite + React) uv Python package management Network access Database on dokploy.voidlabs.cc:5433, S3 on garage-fast.voidlabs.cc Script Options # Start everything (default) ./scripts/dev.sh # Start only frontend ./scripts/dev.sh frontend # Start only backend ./scripts/dev.sh backend # Run migrations only ./scripts/dev.sh migrate # Show help ./scripts/dev.sh help Environment Configuration The script loads environment from: .env - Secrets (API keys, S3 credentials, DB password) .env.development - Dev-specific overrides Storage Configuration Dev mirrors production by using S3 storage: # In .env.development STORAGE_DRIVER=&quot;s3&quot; STORAGE_S3_BUCKET=&quot;efta-images&quot; STORAGE_S3_ENDPOINT=&quot;https://garage-fast.voidlabs.cc&quot; STORAGE_S3_REGION=&quot;garage&quot; # In .env (secrets, gitignored) STORAGE_S3_ACCESS_KEY=&lt;your-key&gt; STORAGE_S3_SECRET_KEY=&lt;your-secret&gt; This ensures dev tests the exact same S3 code paths that production uses. Database Configuration DATABASE_URL=postgresql+psycopg://epstein:changeme_dev@dokploy.voidlabs.cc:5433/epstein Why Native Processes? We use native processes instead of Docker Compose for development because: Faster iteration - No container rebuilds on code changes Hot reload works - Vite and uvicorn both hot-reload Shared dev database - postgres-dev on Dokploy is persistent Simpler debugging - No container layer between you and the code Common Tasks Adding a Python dependency cd backend uv add &lt;package&gt; uv sync Adding a JS dependency cd frontend npm install &lt;package&gt; Running migrations ./scripts/dev.sh migrate # or cd backend &amp;&amp; uv run docviewer-migrate Regenerating OpenAPI client cd frontend npm run generate Troubleshooting &quot;Connection refused&quot; to database The dev database runs on dokploy.voidlabs.cc:5433. Check: Network connectivity to dokploy.voidlabs.cc postgres-dev compose is running in Dokploy S3 Storage Errors If images fail to load, verify S3 configuration: # Test S3 connectivity cd backend source ../.env &amp;&amp; source ../.env.development uv run python -c &quot; from docviewer.core.config import get_settings import boto3 settings = get_settings() s3 = boto3.client('s3', endpoint_url=settings.storage.s3_endpoint, aws_access_key_id=settings.storage.s3_access_key, aws_secret_access_key=settings.storage.s3_secret_key, region_name=settings.storage.s3_region ) response = s3.list_objects_v2(Bucket=settings.storage.s3_bucket, MaxKeys=3) print(f'✓ Connected. Found {response.get(\\&quot;KeyCount\\&quot;, 0)} objects') &quot; Backend won't start cd backend uv sync # Ensure deps are installed uv run docviewer-migrate # Run migrations Frontend build errors cd frontend rm -rf node_modules npm ci Related docviewer-deployment-architecture Voidlabs Devtools dataset-ingestion-workflow"
    },
    {
      "id": "projects/dictaphone-implementation-progress",
      "title": "Dictaphone - Implementation Progress",
      "tags": "projects dictaphone swift implementation",
      "content": "Dictaphone - Implementation Progress Related: Dictaphone App Plan Completed Phases Phase 1: Project Setup &amp; Core Models ✅ Xcode 16 multiplatform project (iOS 18+ / macOS 15+) SwiftData models: Note, Category, Item, AppSettings Enums for provider types and audio retention policies Info.plist permissions configured Phase 2: Audio Recording Infrastructure ✅ AudioRecordingService using AVAudioEngine (cross-platform) Platform-specific audio session handling (iOS: AVAudioSession, macOS: direct AVAudioEngine) SendableAudioData struct for thread-safe audio buffer transfer WaveformView for real-time visualization AudioFileManager for audio file storage Key Decision : Recording format is 44.1kHz mono Float32, streamed via AsyncStream&lt;SendableAudioData&gt; Phase 3: Local Transcription (Apple Speech) ✅ TranscriptionProvider protocol (actor-based for thread safety) AppleSpeechProvider using SFSpeechRecognizer with streaming buffers TranscriptionService facade with @MainActor for UI binding Real-time partial + final results with confidence scores Key Architecture Decisions Swift 6 Strict Concurrency Compliance [decision] All services use proper actor isolation or @MainActor [decision] Non-Sendable types (SFSpeechRecognitionResult) extracted to Sendable structs before crossing actor boundaries [technique] Use ExtractedRecognitionResult pattern to safely pass data from Speech framework callbacks Audio Pipeline Architecture [decision] Recording service emits raw audio as AsyncStream [decision] Transcription provider consumes audio stream, emits result stream [technique] Resampling from 44.1kHz to 16kHz for optimal speech recognition (linear interpolation) Provider Pattern [decision] Protocol-based providers allow swapping local/cloud implementations [decision] Providers are actors to safely manage recognition task state [decision] TranscriptionService facade provides @Published properties for SwiftUI binding Integration Example // Start recording + transcription let audioStream = try await audioService.startRecording() try await transcriptionService.startTranscription(audioStream: audioStream) // Observe via @Published: // - transcriptionService.currentTranscription (includes partial) // - transcriptionService.finalizedTranscription (only confirmed) // - transcriptionService.segments (all finalized segments) Files Structure (Phase 2-3) Core/Services/ ├── Audio/ │ ├── AudioRecordingService.swift # AVAudioEngine wrapper │ └── AudioFileManager.swift # File storage └── Transcription/ ├── TranscriptionTypes.swift # Result types, errors, state ├── TranscriptionProvider.swift # Provider protocol ├── AppleSpeechProvider.swift # SFSpeechRecognizer impl └── TranscriptionService.swift # @MainActor facade Phase 4: Local Categorization (Apple Intelligence) ✅ CategorizationProvider protocol (actor-based, mirrors TranscriptionProvider pattern) AppleIntelligenceProvider using Foundation Models framework @Generable structs for structured AI output ( GeneratedCategorizedItem , GeneratedCategorizationResult ) @Guide attributes for constraining AI output (descriptions, ranges) CategorizationService facade with @MainActor for SwiftUI binding Availability checking via SystemLanguageModel.default.availability Key Decision : @Generable types defined in provider file [decision] @Generable structs are provider-specific, defined in AppleIntelligenceProvider.swift [technique] Foundation Models uses constrained sampling via @Guide attributes [technique] Availability checking handles: deviceNotEligible, appleIntelligenceNotEnabled, modelNotReady Foundation Models Integration Pattern [decision] Use LanguageModelSession().respond(to:generating:) for typed output [decision] Actor isolation for AppleIntelligenceProvider (thread-safe state management) [decision] CategorizationService is @Observable + @MainActor for SwiftUI binding [technique] Build detailed prompts with available categories and custom instructions [technique] Convert @Generable output to domain types (CategorizedItem) for decoupling Provider Availability Flow // Check availability before using let availability = await provider.checkAvailability() switch model.availability { case .available: // ready to use case .unavailable(.deviceNotEligible): // not Apple Silicon case .unavailable(.appleIntelligenceNotEnabled): // user needs to enable case .unavailable(.modelNotReady): // still downloading } Files Structure (Phase 4) Core/Services/Categorization/ ├── CategorizationTypes.swift # Result types, errors, state ├── CategorizationProvider.swift # Provider protocol + factory ├── AppleIntelligenceProvider.swift # Foundation Models impl + @Generable └── CategorizationService.swift # @MainActor facade for UI Integration with Transcription The CategorizationService provides automatic categorization of finalized transcription segments: // Enable auto-categorization categorizationService.defaultCategories = [&quot;Electronics&quot;, &quot;Books&quot;, &quot;Donate&quot;] categorizationService.enableAutoCategorization() // Feed finalized transcription await categorizationService.processFinalizedText(segment.text) // Access results via @Published for item in categorizationService.categorizedItems { print(&quot;\\(item.name) → \\(item.categoryName)&quot;) } Next Phase Phase 5: Core UI - Recording &amp; Note Detail requires Phase 4 categorization for item display implements recording view with live transcription implements note detail view with category sections platform-adaptive layouts (iOS sheet vs macOS panel) drag-drop item reorganization"
    },
    {
      "id": "devops/voidlabs-cicd-automation",
      "title": "Voidlabs CI/CD Automation",
      "tags": "cicd github-actions automation testing releases",
      "content": "Voidlabs CI/CD Automation CI/CD patterns and automation used across voidlabs projects, with examples from the beads CLI tool. GitHub Actions Workflows Standard Pipeline Structure # Typical workflow triggers on: push: branches: [main] tags: ['v*'] pull_request: branches: [main] Common Workflows Workflow Trigger Purpose CI Push/PR Version check, tests, lint, coverage Release Tag push (v*) Multi-platform builds, package publishing Nightly Schedule Full integration tests Test PyPI Manual Staging package verification Testing Strategy Coverage Thresholds coverage: minimum: 45% # Build fails below this warning: 55% # Warning issued below this Test Modes Short tests ( -short flag) - Fast CI feedback Full tests - Nightly integration runs (30-min timeout) Skip patterns - Custom test runner with exclusion support Linting Go : golangci-lint with security-focused rules Python : ruff, mypy General : pre-commit hooks Release Automation Version Synchronization The bump-version.sh script updates version across all files: ./scripts/bump-version.sh 1.2.3 # Updates: version.go, pyproject.toml, package.json, etc. Release Flow # Local release workflow ./scripts/release.sh patch # or minor, major # Automated on tag push: # 1. GoReleaser builds binaries (5 platforms) # 2. PyPI package published # 3. npm package published # 4. Homebrew formula updated Build Targets OS Architecture Linux amd64, arm64 macOS amd64, arm64 Windows amd64 Secrets Management GitHub Secrets PYPI_API_TOKEN # PyPI publishing TEST_PYPI_API_TOKEN # Staging publishing HOMEBREW_TAP_TOKEN # Homebrew formula updates Infrastructure Secrets 1Password integration with op:// references: # In .env or ansible vars api_key: op://vault/item/field Dependabot Configuration # .github/dependabot.yml updates: - package-ecosystem: &quot;gomod&quot; - package-ecosystem: &quot;github-actions&quot; - package-ecosystem: &quot;pip&quot; Ansible Deployment Playbook-Based Deployments # Bootstrap new host ansible-playbook playbooks/bootstrap.yml -l target # Deploy application ansible-playbook playbooks/software/myapp.yml -l target # Sync networking ansible-playbook playbooks/opnsense_dhcp.yml Dynamic Inventory Terraform state → terraform_inventory.py → Ansible groups Best Practices Version Check First - CI validates version consistency early Short Tests for PRs - Full tests run nightly Tag-Based Releases - v* tags trigger full release Multi-Package Sync - Single version across all package managers Secrets via 1Password - No plaintext secrets in repos Related Entries Voidlabs Infrastructure Overview Voidlabs Provisioning Workflow Voidlabs Knowledge Base Plugin"
    },
    {
      "id": "devops/voidlabs-provisioning-workflow",
      "title": "Voidlabs Provisioning Workflow",
      "tags": "provisioning ansible terraform workflow infrastructure",
      "content": "Voidlabs Provisioning Workflow Step-by-step guide for provisioning new infrastructure in the voidlabs homelab. Quick Reference # 1. Create infrastructure (LXC) ./scripts/provision-container.sh myapp --cpu 2 --ram 2048 --clone 9001 # 2. Configure in infrastructure.yml # 3. Sync networking cd ansible &amp;&amp; ansible-playbook playbooks/opnsense_dhcp.yml ansible-playbook playbooks/opnsense_caddy.yml # 4. Bootstrap and install ansible-playbook playbooks/bootstrap.yml -l myapp ansible-playbook playbooks/software/myapp.yml -l myapp Available Templates VMID Type Name Features 9000 LXC voidlabs-debian12-base SSH, chris user, base packages 9001 LXC voidlabs-debian12-docker Base + Docker, fuse enabled 9100 QEMU voidlabs-debian12-cloud Cloud-init, QEMU agent, serial console Detailed Workflow Step 1: Provision the Guest For LXC containers: ./scripts/provision-container.sh myapp \\ --cpu 2 \\ --ram 2048 \\ --clone 9001 \\ # Docker template --full # Full clone (not linked) For VMs: ./scripts/provision-vm.sh myapp \\ --cpu 4 \\ --ram 4096 \\ --clone 9100 Step 2: Configure infrastructure.yml managed_hosts: myapp: reserved_ip: 192.168.51.XX dhcp: true description: &quot;My new service&quot; # Storage mounts mounts: - host: /srv/fast/appdata/myapp container: /data # Reverse proxy caddy: - name: myapp.voidlabs.cc port: 8080 # DNS entries service_domains: - domain: myapp.voidlabs.local target: host Step 3: Sync Networking # Create DHCP reservation and DNS entries ansible-playbook playbooks/opnsense_dhcp.yml # Configure reverse proxy ansible-playbook playbooks/opnsense_caddy.yml Step 4: Bootstrap The bootstrap playbook configures: SSH ED25519 key deployment chris user with passwordless sudo Console autologin (for Proxmox web console) Base packages (curl, wget, vim, git, etc.) Timezone: America/New_York ansible-playbook playbooks/bootstrap.yml -l myapp Step 5: Install Application Create a software playbook or run an existing one: ansible-playbook playbooks/software/myapp.yml -l myapp Dynamic Inventory The terraform_inventory.py script auto-generates host information: # Automatic groups created: - lxc_containers - vms - proxmox_guests # Per-host variables: - ansible_host (IP from DHCP) - ansible_user - proxmox_type (lxc/qemu) - proxmox_vmid Maintenance Playbooks Playbook Purpose pve_create_template.yml Build LXC templates pve_create_vm_template.yml Build cloud-init VM templates pve_rpool_migration.yml Online OS disk migration pve_zpool_stripe.yml Add disk to pool Related Entries Voidlabs Infrastructure Overview Voidlabs Storage Architecture Voidlabs Networking and DNS"
    },
    {
      "id": "infrastructure/sunshine-appsjson-presets-for-multi-device-resolution",
      "title": "Sunshine Apps.json Presets for Multi-Device Resolution",
      "tags": "sunshine moonlight apps.json resolution ipad macbook hyprland headless",
      "content": "Overview Sunshine's apps.json file ( ~/.config/sunshine/apps.json ) defines application presets that appear in Moonlight clients. These can dynamically configure display resolution for different streaming devices. apps.json Location Linux : ~/.config/sunshine/apps.json Edited via Sunshine web UI at https://&lt;host&gt;:47990 or directly Multi-Device Resolution Presets Use prep-cmd to dynamically resize the headless display based on the streaming client device. ⚠️ Important : Sunshine executes prep-cmd directly without a shell. You must wrap commands in bash -c '...' for shell features (pipes, &amp;&amp; , command substitution, export ) to work. Example apps.json with Device Presets ```json { &quot;env&quot;: { &quot;PATH&quot;: &quot;$(PATH):$(HOME)/.local/bin&quot; }, &quot;apps&quot;: [ { &quot;name&quot;: &quot;MacBook Desktop&quot;, &quot;image-path&quot;: &quot;desktop.png&quot;, &quot;prep-cmd&quot;: [ { &quot;do&quot;: &quot;bash -c 'export HYPRLAND_INSTANCE_SIGNATURE=$(ls /run/user/1000/hypr/) &amp;&amp; hyprctl dispatch focusmonitor HEADLESS-1'&quot;, &quot;undo&quot;: &quot;bash -c 'export HYPRLAND_INSTANCE_SIGNATURE=$(ls /run/user/1000/hypr/) &amp;&amp; hyprctl dispatch focusmonitor DP-1'&quot; } ] }, { &quot;name&quot;: &quot;iPad Air 11 Desktop&quot;, &quot;image-path&quot;: &quot;desktop.png&quot;, &quot;prep-cmd&quot;: [ { &quot;do&quot;: &quot;bash -c 'export HYPRLAND_INSTANCE_SIGNATURE=$(ls /run/user/1000/hypr/) &amp;&amp; HEADLESS=$(hyprctl monitors -j | jq -r &quot;[.[] | select(.name | startswith(\\&quot;HEADLESS\\&quot;))] | .[0].name&quot;) &amp;&amp; hyprctl keyword monitor &quot;$HEADLESS,2360x1640@60,auto,1&quot; &amp;&amp; hyprctl dispatch focusmonitor $HEADLESS'&quot;, &quot;undo&quot;: &quot;bash -c 'export HYPRLAND_INSTANCE_SIGNATURE=$(ls /run/user/1000/hypr/) &amp;&amp; HEADLESS=$(hyprctl monitors -j | jq -r &quot;[.[] | select(.name | startswith(\\&quot;HEADLESS\\&quot;))] | .[0].name&quot;) &amp;&amp; hyprctl keyword monitor &quot;$HEADLESS,2880x1864@60,auto,1.5&quot; &amp;&amp; hyprctl dispatch focusmonitor DP-1'&quot; } ] }, { &quot;name&quot;: &quot;Desktop (DP-1)&quot;, &quot;image-path&quot;: &quot;desktop.png&quot; }, { &quot;name&quot;: &quot;Steam Big Picture&quot;, &quot;detached&quot;: [&quot;setsid steam steam://open/bigpicture&quot;], &quot;prep-cmd&quot;: [ { &quot;do&quot;: &quot;&quot;, &quot;undo&quot;: &quot;setsid steam steam://close/bigpicture&quot; } ], &quot;image-path&quot;: &quot;steam.png&quot; } ] } ``` Common Device Resolutions Device Native Resolution Scale Notes MacBook Air 15&quot; 2880×1864 1.5 Retina, use scale for readable UI iPad Air 11&quot; (M3, 2025) 2360×1640 1.0 Native res, no scaling needed iPad Pro 13&quot; 2752×2064 1.0-1.5 Adjust scale to preference Steam Deck 1280×800 1.0 16:10 aspect ratio Key Concepts prep-cmd Structure do : Runs when connecting to this app from Moonlight undo : Runs when disconnecting Dynamic Headless Detection The jq command finds the current headless monitor name: ```bash HEADLESS=$(hyprctl monitors -j | jq -r '[.[] | select(.name | startswith(&quot;HEADLESS&quot;))] | .[0].name') ``` This handles the HEADLESS-N naming which increments during the session. Hyprland Resolution Change ```bash hyprctl keyword monitor &quot;$HEADLESS,2360x1640@60,auto,1&quot; name WxH@Hz pos scale ``` Apply Changes After editing apps.json: ```bash systemctl --user restart sunshine ``` Related Sunshine Streaming Setup"
    },
    {
      "id": "infrastructure/memex-knowledge-base",
      "title": "Memex Knowledge Base",
      "tags": "infrastructure plugins documentation claude-code mcp memex",
      "content": "Memex Knowledge Base Memex is a knowledge management system integrated with Claude Code. It provides persistent, searchable documentation for patterns, decisions, guides, and institutional knowledge. Purpose Capture institutional knowledge - Document decisions, patterns, and lessons learned so they persist across sessions and team members Enable semantic search - Find relevant information using natural language queries, not just keyword matching Build organizational memory - Create a living reference that grows with the organization Core Features Hybrid Search Combines keyword matching with semantic understanding. Search for concepts, not just exact phrases. Bidirectional Links Entries can reference each other using wiki-style [[link]] syntax, creating a connected knowledge graph. Tagging System Consistent taxonomy for categorizing and filtering entries. MCP Integration Exposed as an MCP server, making it accessible to Claude Code sessions automatically. Available Operations Tool Purpose search Find entries using hybrid keyword + semantic search add Create new knowledge base entries update Modify existing entries get Retrieve full entry content list Browse entries by category or tag backlinks Find entries linking to a specific entry reindex Rebuild search indices Best Practices Search before creating - Avoid duplicates by checking existing entries Use consistent tags - Follow the existing taxonomy Add bidirectional links - Connect related concepts Keep entries focused - One concept per entry works better than sprawling docs Technical Details The plugin uses a markdown-based storage format with YAML frontmatter for metadata. Search indices support both traditional keyword matching and vector-based semantic similarity."
    },
    {
      "id": "infrastructure/voidlabs-infrastructure-overview",
      "title": "Voidlabs Infrastructure Overview",
      "tags": "infrastructure proxmox homelab ansible terraform architecture",
      "content": "Voidlabs Infrastructure Overview The voidlabs homelab is a production-grade infrastructure running on Proxmox VE, managed entirely through Infrastructure as Code (IaC) principles. Technology Stack Layer Technology Hypervisor Proxmox VE cluster ( voidlabs ) Provisioning Terraform (Proxmox provider) Configuration Ansible Firewall/Gateway OPNsense Reverse Proxy Caddy (on OPNsense) DNS AdGuard Home + Unbound Storage Multi-tier ZFS + LVM-thin Proxmox Cluster The voidlabs cluster consists of two nodes with asymmetric quorum voting: Node IP Votes Role quasar 192.168.50.2 2 Primary (always-on server) hyperion 192.168.50.147 1 Secondary (dual-boot gaming PC) Quorum configuration: With 3 total votes and quorum of 2, quasar can maintain cluster operations alone if hyperion goes offline. Hyperion cannot operate independently (by design - it's a volatile workstation). Web UI: https://quasar.voidlabs.cc:8006 (manages both nodes) Architecture Flow Infrastructure as Code → Terraform → Proxmox Guests → Ansible → Configured Services ↓ Terraform State ↓ Dynamic Inventory (terraform_inventory.py) Network Layout Network CIDR Purpose Management VLAN 192.168.50.0/24 Proxmox hosts, physical machines Lab VLAN 51 192.168.51.1-127 Guests on quasar Hyperion guests 192.168.51.128-254 Guests on hyperion (routed via OPNsense) Key IPs: OPNsense WAN: 192.168.50.192 (routes hyperion guest traffic) OPNsense LAN/Gateway: 192.168.51.1 DNS: AdGuard (192.168.51.53) Reverse Proxy: Caddy (192.168.51.80) NFS Server: quasar (192.168.50.2) Hyperion guest routing: OPNsense has a static route for 192.168.51.128/25 → 192.168.50.147 (hyperion), allowing guests on hyperion to use .51 IPs without VLAN tagging. Managed Guests LXC Containers adguard (192.168.51.53) - DNS server docker (192.168.51.81) - Docker host with Portainer media (192.168.51.50) - Jellyfin, Arr* stack n8n (192.168.51.89) - Workflow automation searxng (192.168.51.56) - Private search engine qbittorrent (192.168.51.83) - Torrent client typingmind (192.168.51.55) - AI chat interface homebridge (192.168.51.51) - HomeKit bridge QEMU VMs dokploy (192.168.51.87) - Self-hosted PaaS nebula (192.168.51.59) - Home Assistant OS Key Design Principles Single Source of Truth - infrastructure.yml defines all state Automation-First - MAC addresses auto-extracted from Terraform Non-Destructive - Playbooks only manage &quot;Ansible managed&quot; entries API-First - OPNsense managed via REST API State Preservation - Terraform ignores externally-changed attributes Related Entries Voidlabs Storage Architecture Voidlabs Networking and DNS Voidlabs Provisioning Workflow Voidlabs CI/CD Automation Hyperion Proxmox Installation Postmortem"
    },
    {
      "id": "infrastructure/ansible-inventory-split-guide",
      "title": "Ansible Inventory Split Guide",
      "tags": "caddy infrastructure dns homelab docker",
      "content": "Homelab Ansible Directory Structure (2024 Reorganization) The voidlabs-ansible infrastructure has been reorganized for better maintainability. New Directory Layout ansible/inventories/homelab/ ├── infrastructure/ # Split configuration files │ ├── _index.yml # Documentation + quick reference │ ├── network.yml # Lab network settings (rarely edited) │ ├── hosts.yml # Host registry: IPs, DHCP, descriptions │ └── services.yml # DNS + Caddy entries (frequently edited) ├── host_vars/ # Per-host config │ ├── nebula.yml # Home Assistant storage mounts │ ├── dokploy.yml # Dokploy NFS mounts │ ├── docker.yml # Docker config │ └── searxng.yml # SearXNG settings └── group_vars/homelab.yml # Lab-wide variables Quick Reference: What to Edit Task Edit This File Then Run Add new host infrastructure/hosts.yml opnsense_dhcp.yml Add DNS record infrastructure/services.yml opnsense_dhcp.yml Add Caddy proxy infrastructure/services.yml Both dhcp + caddy Storage mounts host_vars/&lt;host&gt;.yml N/A Adding a DNS Record Edit infrastructure/services.yml Find or add host under managed_hosts_services Add to service_domains : docker: service_domains: - domain: newapp.voidlabs.cc target: caddy # Through reverse proxy - domain: direct.voidlabs.local target: host # Direct to host IP Run: ansible-playbook playbooks/opnsense_dhcp.yml Target Options target: host → Host's reserved_ip from hosts.yml target: caddy → Caddy proxy (192.168.51.80) target: &lt;hostname&gt; → Another host's IP target_ip: x.x.x.x → Literal IP override Key IPs Gateway: 192.168.51.1 (opnsense) DNS: 192.168.51.53 (adguard) Caddy: 192.168.51.80"
    },
    {
      "id": "infrastructure/omarchy-vm-setup-on-proxmox-with-gpu-passthrough",
      "title": "Omarchy VM Setup on Proxmox with GPU Passthrough",
      "tags": "proxmox omarchy gpu-passthrough hyperion",
      "content": "Omarchy VM Setup on Proxmox with GPU Passthrough Overview Omarchy is an Arch Linux-based desktop distro by DHH. This documents setting up an Omarchy VM on hyperion (Proxmox node) with NVIDIA GPU passthrough. VM Configuration VMID : 200 RAM : 12GB Disk : 100GB on hyperion-lvm Cores : 8 GPU : NVIDIA RTX 4080 SUPER (passthrough) BIOS : OVMF (UEFI) Machine : q35 Key Proxmox Settings vga: std hostpci0: 01:00.0,pcie=1,rombar=0 hostpci1: 01:00.1,pcie=1 tablet: 1 Important : Use vga: std (not virtio) for UEFI compatibility with web console Use rombar=0 on GPU to prevent UEFI from using physical GPU for boot display Do NOT use x-vga=1 - this would make physical GPU primary and hide LUKS prompt from web console GPU Passthrough Prerequisites See infrastructure/proxmox-gpu-passthrough-on-amd-kernel-68-iommu-issue.md for kernel workaround on AMD hosts. Post-Install Issues Missing Default Route Omarchy DHCP client sometimes fails to set default gateway: ip route | grep default # Check if missing sudo ip route add default via 192.168.50.1 Firewall Blocks Outbound UFW is enabled by default: sudo ufw default allow outgoing sudo ufw allow ssh sudo ufw reload Hyprland Rendering to Wrong GPU In multi-GPU VMs, Hyprland may render to virtual GPU (bochs) instead of NVIDIA. Add to ~/.config/hypr/hyprland.conf: env = WLR_DRM_DEVICES,/dev/dri/card0 env = AQ_DRM_DEVICES,/dev/dri/card0 Remote Access with Sunshine For GPU-accelerated remote desktop streaming via Moonlight, see: infrastructure/sunshine-streaming-setup-on-omarchy-vm-with-nvidia-gpu-passthrough.md Ansible Playbooks pve_iommu_passthrough.yml - Enable IOMMU and VFIO pve_create_omarchy_vm.yml - Create the VM with GPU passthrough pve_post_install.yml - Remove subscription nag, configure repos Notes USB tablet device required for proper mouse alignment in web console Guest agent not available during Omarchy ISO boot Omarchy requires disk encryption during installation (LUKS) LUKS password must be entered via web console after each reboot"
    },
    {
      "id": "infrastructure/virtiofs-limitations-and-troubleshooting",
      "title": "VirtioFS Limitations and Troubleshooting",
      "tags": "infrastructure virtiofs nfs troubleshooting proxmox storage",
      "content": "VirtioFS Limitations and Troubleshooting VirtioFS is Proxmox's high-performance shared folder solution for QEMU VMs, but it has limitations that can cause failures for certain workloads. Known Issues IO Error 95: Operation Not Supported Symptom: Error: IO error: Not supported (os error 95) Files and directories appear to exist (visible with ls , find , etc.) but operations fail with &quot;not supported&quot; errors. Root Cause: VirtioFS implements a limited subset of POSIX filesystem features. It lacks support for: Extended attributes (xattrs) Certain ioctls File locking mechanisms required by some applications Some advanced filesystem operations Affected Workloads: Garage S3 Storage - Requires full POSIX semantics for its storage backend Database engines - May use file locking or xattrs Container runtimes - Docker/Podman may need xattrs for overlay filesystems Any application using setxattr() / getxattr() system calls Example Error Pattern: # Directory appears to exist $ ls /mnt/virtiofs/garage/data metadata/ data/ # But operations fail $ garage server Error: IO error: Not supported (os error 95) Solutions Solution 1: Use NFS Mounts Instead For workloads requiring full POSIX capabilities, use NFS instead of virtiofs. Configuration in infrastructure.yml : managed_hosts: myvm: type: vm nfs_mounts: - source: quasar:/srv/fast-nfs/appdata/myapp target: /mnt/fast-nfs options: &quot;rw,soft,intr,timeo=30&quot; Manual Mount: # Create mount point sudo mkdir -p /mnt/fast-nfs # Mount NFS share sudo mount -t nfs 192.168.50.2:/srv/fast-nfs /mnt/fast-nfs \\ -o rw,soft,intr,timeo=30 # Verify df -h | grep nfs Make Permanent: Add to /etc/fstab : 192.168.50.2:/srv/fast-nfs /mnt/fast-nfs nfs rw,soft,intr,timeo=30 0 0 Solution 2: Use Local VM Storage For truly critical workloads, consider using local VM disk images instead of shared storage. When to Use Each Option Storage Type Best For Avoid For VirtioFS Read-heavy workloads, code repositories, general file sharing Databases, object storage, applications using xattrs NFS Full POSIX compliance needed, Garage S3, databases Extremely latency-sensitive workloads Local Disk Maximum performance and isolation Scenarios requiring shared access from host Available NFS Mounts at Voidlabs Mount Point Source Purpose /mnt/fast-nfs 192.168.50.2:/srv/fast-nfs Performance-sensitive shared data /mnt/slow-nfs 192.168.50.2:/srv/slow-nfs Bulk/archival shared data Both NFS exports are created via bindfs overlays with proper UID/GID mapping for the storage group. Debugging Steps Check if virtiofs is mounted: mount | grep virtiofs Test file operations: # Try to create a file with xattrs touch /mnt/virtiofs/test.txt setfattr -n user.test -v &quot;value&quot; /mnt/virtiofs/test.txt If this fails with &quot;Operation not supported&quot;, the application likely needs NFS. Check application logs: Look for error messages mentioning: ENOTSUP (operation not supported) error 95 xattr , ioctl , or file locking failures Compare with NFS: # Test same operation on NFS mount touch /mnt/fast-nfs/test.txt setfattr -n user.test -v &quot;value&quot; /mnt/fast-nfs/test.txt Migration Pattern: VirtioFS to NFS Example: Migrating Garage S3 from virtiofs to NFS. 1. Stop the application: cd /path/to/docker-compose docker-compose down 2. Ensure NFS is mounted: sudo mount -t nfs 192.168.50.2:/srv/fast-nfs /mnt/fast-nfs -o rw,soft,intr,timeo=30 3. Update docker-compose.yml: # Before (virtiofs) volumes: - /srv/fast/garage/data:/data # After (NFS) volumes: - /mnt/fast-nfs/garage/data:/data 4. Migrate data if needed: # Copy from virtiofs to NFS location sudo rsync -av /srv/fast/garage/ /mnt/fast-nfs/garage/ 5. Restart application: docker-compose up -d Performance Considerations VirtioFS advantages: Lower latency for metadata operations Better small file performance No network overhead NFS advantages: Full POSIX compliance Better large file streaming Easier to debug (standard protocol) Can be accessed from multiple VMs simultaneously For most applications, the performance difference is negligible compared to the compatibility benefits of NFS. Related Entries Voidlabs Storage Architecture Garage S3 Object Storage Voidlabs Infrastructure Overview"
    },
    {
      "id": "infrastructure/proxmox-vm-filesystem-recovery-with-fsck",
      "title": "Proxmox VM Filesystem Recovery with fsck",
      "tags": "proxmox filesystem recovery fsck ext4 troubleshooting vm read-only journal disk-repair qm zfs zvol e2fsck corruption",
      "content": "Proxmox VM Filesystem Recovery with fsck Symptoms VM shows EXT4 filesystem errors and goes read-only: EXT4-fs error (device sda1): ext4_journal_check_start:83: Detected aborted journal EXT4-fs (sda1): Remounting filesystem read-only Services fail with &quot;read-only file system&quot; errors. Recovery Steps 1. Stop the VM ssh root@&lt;proxmox-host&gt; qm stop &lt;vmid&gt; 2. Find the disk device # Check VM config for disk qm config &lt;vmid&gt; | grep -E '(scsi|virtio|ide|sata).*disk' # Example: scsi0: guests:vm-118-disk-0,discard=on,size=150G,ssd=1 # Find the partition devices ls -la /dev/zvol/guests/ | grep &lt;vmid&gt; # Look for: vm-118-disk-0-part1 (root), vm-118-disk-0-part14 (BIOS), vm-118-disk-0-part15 (EFI) 3. Run fsck # Force full check with auto-repair sudo e2fsck -f -y /dev/zvol/guests/vm-&lt;vmid&gt;-disk-0-part1 Important : Use -f to force full check even if filesystem appears clean. A basic fsck -y may not catch all issues. 4. Start the VM qm start &lt;vmid&gt; 5. Verify ssh user@&lt;vm&gt; mount | grep ' / ' # Should show 'rw' not 'ro' dmesg | grep -i 'ext4\\|error' # Should be clean Common Issues &quot;Device is mounted&quot; error If fsck says device is mounted, check for stale mounts from previous recovery attempts: mount | grep &lt;vmid&gt; umount /mnt/vm&lt;vmid&gt; Services fail after recovery Docker Swarm may need reinitialization: docker swarm init --force-new-cluster Dokploy isolated apps may need redeployment (see dokploy-network-recovery ). Related dokploy-network-recovery ## Search Keywords read-only file system, aborted journal, filesystem corruption, ext4 error, remounting read-only, disk repair, vm won't boot, emergency mode"
    },
    {
      "id": "infrastructure/sunshine-streaming-setup-on-omarchy-vm-with-nvidia-gpu-passthrough",
      "title": "Sunshine Streaming Setup on Omarchy VM with NVIDIA GPU Passthrough",
      "tags": "sunshine moonlight nvidia gpu-passthrough proxmox omarchy streaming wayland hyprland headless",
      "content": "Sunshine Streaming Setup on Omarchy VM with NVIDIA GPU Passthrough Overview Setting up Sunshine game streaming on an Omarchy (Arch Linux) VM with NVIDIA GPU passthrough requires careful configuration of multiple components. This guide documents the complete setup process and troubleshooting steps. Prerequisites Proxmox VE host with NVIDIA GPU passed through to VM Omarchy/Arch Linux VM with LUKS encryption NVIDIA drivers installed in the VM Physical monitors connected to the passed-through GPU (or headless setup) Key Configuration Files 1. Proxmox VM Configuration For web console access during LUKS decryption: vga: std hostpci0: 01:00.0,pcie=1,rombar=0 hostpci1: 01:00.1,pcie=1 Critical : Use vga: std (not virtio) and rombar=0 on GPU to ensure UEFI uses the virtual display for boot/LUKS prompt, not the physical GPU. 2. Hyprland Configuration (~/.config/hypr/hyprland.conf) Force NVIDIA GPU for rendering (critical for multi-GPU VMs): # Force NVIDIA GPU for rendering env = WLR_DRM_DEVICES,/dev/dri/card0 env = AQ_DRM_DEVICES,/dev/dri/card0 Without this, Hyprland may render to the virtual GPU (bochs) instead of NVIDIA, causing black screens on physical monitors and in Moonlight. 3. Sunshine Configuration (~/.config/sunshine/sunshine.conf) For headless/virtual display streaming (recommended): # Stream from headless display # Use wlr capture mode - required for headless displays capture = wlr # Monitor index - use startup script to auto-detect output_name = 2 For physical monitor streaming: # Monitor selection (indices differ between capture modes!) # Check logs: journalctl --user -u sunshine | grep &quot;Monitor list&quot; output_name = 1 capture = kms adapter_name = /dev/dri/card0 4. Sunshine Systemd Override (~/.config/systemd/user/sunshine.service.d/wayland.conf) [Service] Environment=&quot;WAYLAND_DISPLAY=wayland-1&quot; Environment=&quot;XDG_RUNTIME_DIR=/run/user/1000&quot; Headless Display Setup (for Remote-Only Streaming) For streaming without physical monitors attached, use Hyprland headless displays. Hyprland Headless Configuration Add to ~/.config/hypr/hyprland.conf : # Headless display config - cover multiple names since they increment per session # Resolution matches MacBook Air 15&quot; (2880x1864), scale 1.5 for readable UI monitor = HEADLESS-1,2880x1864@60,auto,1.5 monitor = HEADLESS-2,2880x1864@60,auto,1.5 monitor = HEADLESS-3,2880x1864@60,auto,1.5 monitor = HEADLESS-4,2880x1864@60,auto,1.5 monitor = HEADLESS-5,2880x1864@60,auto,1.5 monitor = HEADLESS-6,2880x1864@60,auto,1.5 monitor = HEADLESS-7,2880x1864@60,auto,1.5 monitor = HEADLESS-8,2880x1864@60,auto,1.5 monitor = HEADLESS-9,2880x1864@60,auto,1.5 monitor = HEADLESS-10,2880x1864@60,auto,1.5 # Make headless the primary display (workspace 1) workspace = 1, monitor:HEADLESS-1, default:true # Software cursor - required for cursor to appear in headless streams cursor:no_hardware_cursors = true # Auto-configure headless on startup exec-once = sleep 2 &amp;&amp; ~/.local/bin/configure-headless.sh Why multiple HEADLESS-N entries? Hyprland increments the headless number each time one is created during a session. On reboot, it resets to HEADLESS-1. Startup Script (~/.local/bin/configure-headless.sh) #!/bin/bash # Configure headless monitor for Moonlight streaming sleep 3 # Wait for Hyprland to fully initialize export HYPRLAND_INSTANCE_SIGNATURE=$(ls /run/user/1000/hypr/ 2&gt;/dev/null) if [ -z &quot;$HYPRLAND_INSTANCE_SIGNATURE&quot; ]; then exit 0 fi # Create headless if none exists if ! hyprctl monitors -j | jq -e &quot;.[] | select(.name | startswith(\\&quot;HEADLESS\\&quot;))&quot; &gt; /dev/null 2&gt;&amp;1; then hyprctl output create headless sleep 1 fi # Find headless monitor and get its index HEADLESS_INFO=$(hyprctl monitors -j | jq -r &quot;.[] | select(.name | startswith(\\&quot;HEADLESS\\&quot;)) | \\&quot;\\(.name) \\(.id)\\&quot;&quot; | head -1) HEADLESS_NAME=$(echo &quot;$HEADLESS_INFO&quot; | cut -d&quot; &quot; -f1) HEADLESS_ID=$(echo &quot;$HEADLESS_INFO&quot; | cut -d&quot; &quot; -f2) if [ -n &quot;$HEADLESS_NAME&quot; ]; then # Configure resolution hyprctl keyword monitor &quot;$HEADLESS_NAME,2880x1864@60,auto,1.5&quot; # Update Sunshine config with correct index cat &gt; ~/.config/sunshine/sunshine.conf &lt;&lt; SCONF # Stream from headless display (auto-configured) output_name = $HEADLESS_ID capture = wlr SCONF # Restart Sunshine to pick up new config systemctl --user restart sunshine fi Make executable: chmod +x ~/.local/bin/configure-headless.sh Key Insights: wlr vs kms Capture Feature kms capture wlr capture Physical monitors Yes Yes Headless displays No Yes Hardware cursor Yes No (use software cursor) Performance Better Good Critical : Use capture = wlr for headless displays. KMS capture cannot see headless outputs. Monitor Index Instability The monitor index order is not stable between reboots or even Sunshine restarts. The order depends on when displays are detected. Solution : The startup script auto-detects the headless monitor index and updates sunshine.conf dynamically. Required Packages # NVIDIA drivers and CUDA (for optimal NVENC performance) sudo pacman -S nvidia nvidia-utils cuda # Sunshine (from AUR) yay -S sunshine # jq for startup script sudo pacman -S jq Critical: Set Capabilities for Capture This step is required for Wayland/KMS capture and input to work: # Set cap_sys_admin on Sunshine binary sudo setcap cap_sys_admin+p $(readlink -f $(which sunshine)) # Verify getcap $(which sunshine) # Should show: cap_sys_admin=p Without this capability, capture will fail silently and input devices won't work. Required Group Memberships sudo usermod -aG video,render,input chris video : Access to /dev/dri/card* render : Access to /dev/dri/renderD* input : Access to /dev/uinput for virtual keyboard/mouse Note : Group changes require logout/login or reboot to take effect. Firewall Ports (UFW) sudo ufw allow 47984/tcp # RTSP sudo ufw allow 47989/tcp # HTTPS/control sudo ufw allow 47990/tcp # Web UI sudo ufw allow 47998:48010/tcp # Video/audio sudo ufw allow 47998:48010/udp # Video/audio sudo ufw allow 47999/udp # Control sudo ufw allow 48000/udp # Audio Troubleshooting Black Screen in Moonlight Check if Hyprland is rendering to NVIDIA GPU : hyprctl systeminfo | grep -i panel All panels should show &quot;backend drm&quot; Test screencopy : WAYLAND_DISPLAY=wayland-1 grim -o DP-1 /tmp/test.png file /tmp/test.png # Should show actual size, not tiny If screenshot is black (0 bytes content) : Add WLR_DRM_DEVICES to hyprland.conf and restart session For headless : Ensure using capture = wlr not capture = kms Headless Display Reverts to 1920x1080 Hyprland headless displays default to 1920x1080. If your custom resolution doesn't stick: Add config entries for ALL possible HEADLESS-N names (1-10) Run hyprctl reload after changes Use the startup script to auto-configure Monitor Index Changes If Sunshine streams from wrong monitor after restart: Check current order: journalctl --user -u sunshine | grep &quot;Monitor list&quot; Update output_name in sunshine.conf Use the startup script for automatic detection NVENC Not Working Install CUDA: sudo pacman -S cuda Check logs for &quot;NVENC without CUDA support&quot; warning Verify encoder detection: journalctl --user -u sunshine | grep encoder No Keyboard/Mouse Input Verify input group: groups $USER | grep input Check /dev/uinput permissions: ls -la /dev/uinput Verify cap_sys_admin: getcap $(which sunshine) Restart Sunshine after adding to input group Cursor Not Visible (Headless) Add to hyprland.conf: cursor:no_hardware_cursors = true Then reload: hyprctl reload Sunshine Not Starting After Reboot Sunshine may start before Hyprland initializes all displays: systemctl --user restart sunshine Network Issues After Reboot Omarchy may lose default gateway: sudo ip route add default via 192.168.50.1 Verification Commands # Check NVIDIA driver nvidia-smi # Check Hyprland monitors (including headless) export HYPRLAND_INSTANCE_SIGNATURE=$(ls /run/user/1000/hypr/) hyprctl monitors # Check Sunshine status systemctl --user status sunshine # View Sunshine logs journalctl --user -u sunshine -f # Check what monitor Sunshine selected journalctl --user -u sunshine | grep &quot;Selected monitor&quot; # Test screenshot capture WAYLAND_DISPLAY=wayland-1 grim -o HEADLESS-1 /tmp/test.png Known Issues Cursor may not display : Use cursor:no_hardware_cursors = true for headless Session restart required : After group changes or GPU config changes LUKS prompt on wrong display : Without rombar=0, UEFI may use physical GPU HEADLESS-N increments : Names increment during session, reset on reboot Monitor index unstable : Use startup script for automatic detection Related infrastructure/proxmox-gpu-passthrough-on-amd-kernel-68-iommu-issue.md infrastructure/omarchy-vm-setup-on-proxmox-with-gpu-passthrough.md"
    },
    {
      "id": "infrastructure/common-patterns",
      "title": "Voidlabs Common Patterns",
      "tags": "infrastructure patterns best-practices developer-guide",
      "content": "Voidlabs Common Patterns Standard patterns and conventions used across voidlabs projects. Project Structure Pattern All voidlabs projects follow a consistent structure: project-name/ ├── AGENTS.md # AI agent guidance (standard template) ├── README.md # Project documentation ├── .beads/ # Beads issue tracker ├── .claude/ # Claude Code settings ├── .devcontainer/ # Devcontainer configuration │ ├── devcontainer.json │ ├── Dockerfile │ ├── voidlabs.conf # Feature toggles │ └── scripts/ │ ├── post-start-common.sh │ └── post-start-project.sh ├── backend/ # FastAPI backend (Python projects) │ ├── pyproject.toml # UV-managed dependencies │ └── src/{project}/ ├── frontend/ # Vite + React frontend │ └── src/ └── docs/ # Project documentation └── ARCHITECTURE.md Configuration Pattern Layered Configuration with Phase All projects use a consistent configuration pattern: .env.defaults # Base config (committed) - non-secrets .env.development # Dev overrides (committed) .env.staging # Staging overrides (committed) .env.production # Production overrides (committed) .env # Secrets only (git-ignored) - from Phase Precedence (highest to lowest): Environment variables (Phase shell export) .env (secrets file) .env.{ENVIRONMENT} (environment-specific) .env.defaults (base config) Field defaults in code Phase Integration # Environment setup export PHASE_HOST=&quot;https://secrets.voidlabs.cc&quot; export PHASE_SERVICE_TOKEN=&quot;pss_user:v1:...&quot; export PHASE_ENV=&quot;development&quot; # or staging, production # In devcontainers, Phase exports secrets automatically # via post-start-common.sh Backend Stack Pattern FastAPI + SQLAlchemy + Pydantic Standard backend structure: backend/src/{project}/ ├── api/v1/endpoints/ # FastAPI routes ├── cli/ # Typer CLI commands ├── core/ # Config, DB setup, logging ├── schemas/ # Pydantic models └── services/ # Business logic Dependencies via UV: cd backend uv sync uv run uvicorn {project}.main:app --reload Common CLI Pattern Projects expose Typer CLIs as entry points: # pyproject.toml [project.scripts] docviewer-migrate = &quot;docviewer.cli.migrate:app&quot; docviewer-embed = &quot;docviewer.cli.embed:app&quot; Frontend Stack Pattern Vite + React + TypeScript + Tailwind Standard frontend structure: frontend/ ├── src/ │ ├── components/ # Reusable UI components │ ├── pages/ # Route pages │ ├── hooks/ # Custom React hooks │ ├── services/ # API client │ └── types/ # TypeScript types ├── package.json ├── vite.config.ts └── tailwind.config.js Development: cd frontend npm install npm run dev -- --host 0.0.0.0 --port 5173 Storage Pattern StorageService Abstraction Projects use a unified storage service supporting multiple backends: # Supports both local:// and s3:// URIs storage.store(file, &quot;local://path/to/file.png&quot;) storage.store(file, &quot;s3://bucket/key.png&quot;) storage.get(&quot;local://path/to/file.png&quot;) Configuration: STORAGE_BACKEND=local # or s3 STORAGE_LOCAL_PATH=/data/files STORAGE_S3_BUCKET=my-bucket STORAGE_S3_ENDPOINT=https://s3.amazonaws.com Deployment Pattern Dokploy All production deployments use Dokploy at dokploy.voidlabs.cc : Projects: Each app gets its own Dokploy project Dependencies: Database, Qdrant, etc. as separate services Secrets: Injected via Phase environment variables Port Conventions Service Dev Port Container Port Backend (FastAPI) 8000 8000 Frontend (Vite) 5173 5173 PostgreSQL 5432/5433 5432 Qdrant 6333/6334 6333 GPU Processing Pattern Hyperion Server GPU workloads run on Hyperion (RTX 4080 Super): # Embedding server (llama.cpp) http://hyperion.voidlabs:8080/v1/embeddings # OCR worker CUDA_VISIBLE_DEVICES=0 uv run {project}-ocr paddle-worker Configuration: EMBEDDING_PROVIDER=text_embeddings_inference EMBEDDING_URL=http://hyperion:8080/v1/embeddings Issue Tracking Pattern Beads Workflow All projects use beads for issue tracking: # Initialize bd init # Workflow bd ready # Find unblocked work bd update &lt;id&gt; --status in_progress # ... do work ... bd close &lt;id&gt; --reason &quot;Completed&quot; bd sync # Commit and push Session start: bd prime provides context to AI agents. Testing Pattern Backend (pytest) uv run pytest uv run pytest --cov Frontend (Vitest/Jest) npm run test npm run lint CI Commands uv run ruff check npm run build Related Entries Voidlabs Infrastructure Overview Voidlabs Devtools Phase Secrets Management Beads Issue Tracker"
    },
    {
      "id": "infrastructure/voidlabs-storage-architecture",
      "title": "Voidlabs Storage Architecture",
      "tags": "storage zfs nfs virtiofs infrastructure",
      "content": "Voidlabs Storage Architecture The voidlabs infrastructure uses a multi-tier ZFS storage model with different access patterns for containers vs VMs. Storage Tiers Tier Host Path Purpose Hardware fast /srv/fast Performance-sensitive data NVMe SSDs slow /srv/slow Bulk data, archives, object storage HDDs Access Patterns LXC Containers Containers use bind mounts for direct filesystem access: mounts: - host: /srv/fast/appdata/myapp container: /data QEMU VMs VMs have dual access methods: NFS Mounts - Traditional file sharing Export path: /srv/fast-nfs , /srv/slow-nfs Uses bindfs overlay for UID mapping Recommended for workloads requiring full POSIX compliance (Garage S3, databases, applications using xattrs) virtiofs - High-performance shared folders Proxmox native feature Better performance than NFS for local VMs Limitations: Does not support extended attributes (xattrs), certain ioctls, or advanced file locking See VirtioFS Limitations and Troubleshooting for known issues and solutions User/Permission Mapping # LXC unprivileged container range container_uid: 101000 container_gid: 101000 # VM access via storage group storage_group_gid: 101000 The chris user is automatically added to the storage group on VMs for seamless access. Bindfs Overlays Bindfs creates FUSE-based overlay mounts that handle UID/GID translation: /srv/fast (host UIDs) → bindfs → /srv/fast-nfs (mapped UIDs) → NFS export This allows VMs with different UID schemes to access the same underlying storage. Mount Configuration Storage mounts are defined in infrastructure.yml : managed_hosts: myapp: mounts: - host: /srv/fast/appdata/myapp container: /data readonly: false nfs_mounts: # For VMs - source: quasar:/srv/fast-nfs/appdata target: /mnt/fast Related Playbooks bindfs_overlays.yml - Create FUSE overlay mounts nfs_mounts.yml - Configure VM NFS access virtiofs_mounts.yml - Configure VM shared folders Related Entries Voidlabs Infrastructure Overview Voidlabs Provisioning Workflow VirtioFS Limitations and Troubleshooting Garage S3 Object Storage"
    },
    {
      "id": "infrastructure/hyperion-proxmox-boot-failure-postmortem",
      "title": "Hyperion Proxmox Installation Boot Failure Postmortem",
      "tags": "proxmox hyperion infrastructure troubleshooting postmortem",
      "content": "Hyperion Proxmox Installation Boot Failure Postmortem Context Hyperion is a dual-boot Windows/Debian machine being converted to Proxmox VE. Issue Encountered (2026-01) After installing Proxmox VE and configuring LVM thin pool + network bridge via CLI, the system became unbootable. Symptoms: Boot hangs after showing /dev/mapper/pve-root lines Even basic.target hangs at same point initramfs shell shows: dmeventd: stat failed and failed to monitor pve/data Removing thin pool and disabling monitoring didn't fix it Root cause never definitively identified What We Did Before the Hang Installed Proxmox via ansible playbook (worked, rebooted fine) Created vmbr0 bridge by editing /etc/network/interfaces directly Created LVM thin pool via CLI: lvcreate -L 390G -T pve/data Added storage to Proxmox via API Changed EFI boot order with efibootmgr -o Rebooted → unbootable Lessons / Best Practices DO: Use Proxmox Web UI for Post-Install Configuration Network bridges : Datacenter → hyperion → Network → Create → Linux Bridge LVM-Thin storage : Datacenter → Storage → Add → LVM-Thin The web UI handles systemd integration, udev rules, and config syntax correctly DO: Reboot After Each Major Change Reboot after Proxmox kernel install (playbook does this) Reboot after creating network bridge Reboot after adding storage Catch issues early before stacking changes DON'T: Stack Multiple Infrastructure Changes Before Rebooting We made bridge, LVM, storage, and EFI changes all before one reboot Made it impossible to identify which change broke things DON'T: Manually Edit /etc/network/interfaces on Proxmox Proxmox has its own network management Use the web UI or /etc/network/interfaces.d/ snippets Ansible Playbook Notes Playbook location: voidlabs-ansible/ansible/playbooks/infrastructure/pve_install_on_debian.yml Inventory: voidlabs-ansible/ansible/inventories/homelab/11-hyperion.yml Uses su become method (sudo not installed on fresh Debian) Run from devcontainer: docker exec &lt;container&gt; ansible-playbook ... Network Configuration Hyperion IP: 192.168.50.147 Gateway: 192.168.50.1 Guests will use 192.168.51.128/25 range (OPNsense route configured) Resolution (2026-01-02) Reinstalled Debian and followed the lessons above: Ran Proxmox install playbook → rebooted → verified boot ✓ Ran post-install playbook (disable enterprise repos) ✓ Ran bootstrap playbook (sudo, SSH keys, packages) ✓ Created vmbr0 bridge via Proxmox web UI → rebooted → verified ✓ Created LVM thin pool via CLI, added storage via web UI → rebooted → verified ✓ Joined hyperion to voidlabs cluster with quasar (2 votes quasar, 1 vote hyperion) Current status: Fully operational, accessible via cluster UI at https://quasar.voidlabs.cc:8006 Related Voidlabs Infrastructure Overview"
    },
    {
      "id": "infrastructure/proxmox-subscription-nag-removal",
      "title": "Proxmox Subscription Nag Removal",
      "tags": "proxmox configuration",
      "content": "Proxmox Subscription Nag Removal Problem Proxmox VE shows a subscription popup on login when using the free/community version. Solution Patch the JavaScript that checks for subscription: # Patch the subscription check sed -Ei.bak &quot;s/res\\.data\\.status\\.toLowerCase\\(\\) !== .active./false/&quot; \\ /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js # Restart the proxy service systemctl restart pveproxy Notes This patch may need to be reapplied after Proxmox updates The enterprise repo should also be disabled: # Disable enterprise repo echo &quot;# Disabled - requires subscription&quot; &gt; /etc/apt/sources.list.d/pve-enterprise.list # Add no-subscription repo echo &quot;deb [arch=amd64] http://download.proxmox.com/debian/pve bookworm pve-no-subscription&quot; \\ &gt; /etc/apt/sources.list.d/pve-no-subscription.list Ansible Playbook See: playbooks/infrastructure/pve_post_install.yml This playbook handles: Disabling enterprise repositories Adding no-subscription repository Removing subscription nag popup Updating package cache"
    },
    {
      "id": "infrastructure/garage-s3-object-storage",
      "title": "Garage S3 Object Storage",
      "tags": "infrastructure s3 garage object-storage dokploy",
      "content": "Garage S3 Object Storage Garage is a self-hosted, S3-compatible distributed object storage system running on dokploy.voidlabs.cc. Instances Instance Endpoint Storage Use Case garage-fast https://garage-fast.voidlabs.cc NVMe pool High-performance workloads garage-slow https://garage-slow.voidlabs.cc HDD pool Archival/cold storage Both are managed via Dokploy with WebUI access available. Architecture Garage separates concerns into two API layers: Admin API - Bucket/key management (requires admin token or CLI access) S3 API - Object operations (uses standard S3 access keys) Storage Backend Requirements Garage requires full POSIX filesystem semantics for its data directory: Extended attributes (xattrs) support Proper file locking Complete ioctl support Important: VirtioFS does not support these features and will fail with IO error: Not supported (os error 95) . Use NFS mounts instead. See VirtioFS Limitations and Troubleshooting for details. AWS CLI Configuration # Configure a profile for Garage aws configure set aws_access_key_id &quot;YOUR_ACCESS_KEY&quot; --profile garage aws configure set aws_secret_access_key &quot;YOUR_SECRET_KEY&quot; --profile garage aws configure set region &quot;garage&quot; --profile garage # Use with --endpoint-url aws --profile garage --endpoint-url https://garage-fast.voidlabs.cc s3 ls Bucket Management (Admin Operations) Access keys cannot create buckets. Use the Garage CLI inside the container: # Create bucket ssh dokploy.voidlabs.cc &quot;docker exec voidlabssharedservices-garagewithui-vo0uq2-garage-fast-1 \\ /garage bucket create my-bucket&quot; # Grant access to a key ssh dokploy.voidlabs.cc &quot;docker exec voidlabssharedservices-garagewithui-vo0uq2-garage-fast-1 \\ /garage bucket allow --read --write my-bucket --key GK...&quot; # List buckets ssh dokploy.voidlabs.cc &quot;docker exec voidlabssharedservices-garagewithui-vo0uq2-garage-fast-1 \\ /garage bucket list&quot; # Show bucket info ssh dokploy.voidlabs.cc &quot;docker exec voidlabssharedservices-garagewithui-vo0uq2-garage-fast-1 \\ /garage bucket info my-bucket&quot; Object Operations (S3 API) Standard S3 operations work with the endpoint URL: # Upload file aws --profile garage --endpoint-url https://garage-fast.voidlabs.cc \\ s3 cp myfile.pdf s3://my-bucket/ # Sync directory aws --profile garage --endpoint-url https://garage-fast.voidlabs.cc \\ s3 sync ./local-dir s3://my-bucket/prefix/ # List objects aws --profile garage --endpoint-url https://garage-fast.voidlabs.cc \\ s3 ls s3://my-bucket/ Python/boto3 Usage import boto3 s3 = boto3.client( &quot;s3&quot;, endpoint_url=&quot;https://garage-fast.voidlabs.cc&quot;, aws_access_key_id=&quot;GK...&quot;, aws_secret_access_key=&quot;...&quot;, region_name=&quot;garage&quot; ) # Upload s3.upload_file(&quot;local.pdf&quot;, &quot;my-bucket&quot;, &quot;remote.pdf&quot;) # Download s3.download_file(&quot;my-bucket&quot;, &quot;remote.pdf&quot;, &quot;local.pdf&quot;) # Generate presigned URL (for web access) url = s3.generate_presigned_url( &quot;get_object&quot;, Params={&quot;Bucket&quot;: &quot;my-bucket&quot;, &quot;Key&quot;: &quot;file.pdf&quot;}, ExpiresIn=3600 ) Container Names For admin operations, the relevant containers on dokploy are: voidlabssharedservices-garagewithui-vo0uq2-garage-fast-1 - Fast tier Garage voidlabssharedservices-garagewithui-vo0uq2-garage-slow-1 - Slow tier Garage voidlabssharedservices-garagewithui-vo0uq2-garage-webui-fast-1 - Fast WebUI voidlabssharedservices-garagewithui-vo0uq2-garage-webui-slow-1 - Slow WebUI Volume Configuration Garage data directories must be on NFS mounts , not virtiofs: # docker-compose.yml services: garage-fast: volumes: # Use NFS mount path - /mnt/fast-nfs/garage/data:/data # NOT virtiofs path like /srv/fast/garage/data Setup NFS mount on dokploy VM: # Mount fast tier storage sudo mount -t nfs 192.168.50.2:/srv/fast-nfs /mnt/fast-nfs \\ -o rw,soft,intr,timeo=30 # Mount slow tier storage sudo mount -t nfs 192.168.50.2:/srv/slow-nfs /mnt/slow-nfs \\ -o rw,soft,intr,timeo=30 Make permanent by adding to /etc/fstab or configuring via infrastructure.yml . Existing Buckets Bucket Purpose Access Key efta-images EFTA case document images docviewer key See Also Voidlabs Infrastructure Overview Voidlabs Storage Architecture VirtioFS Limitations and Troubleshooting Dokploy Container Management ### Storage Backend Requirements Storage Backend Requirements Garage requires full POSIX filesystem semantics for its data directory: Extended attributes (xattrs) support Proper file locking Complete ioctl support CRITICAL: VirtioFS does not support these features and will fail with: IO error: Not supported (os error 95) on xattr operations Could not find expected marker file 'garage-marker' after container restart Always use NFS mounts , not virtiofs: Mount Type Path Pattern Works? NFS /mnt/fast-nfs/... ✅ Yes VirtioFS /mnt/fast/... ❌ No See VirtioFS Limitations and Troubleshooting for details. ### Volume Configuration Volume Configuration Garage data directories MUST be on NFS mounts, not virtiofs: # docker-compose.yml services: garage-fast: volumes: # ✅ CORRECT: Use NFS mount path - /mnt/fast-nfs/dokploy.voidlabs.cc/voidlabs-shared-services/garage/data:/var/lib/garage/data # ❌ WRONG: VirtioFS path will fail # - /mnt/fast/dokploy.voidlabs.cc/.../garage/data:/var/lib/garage/data Verify NFS is mounted on dokploy VM: # Check if NFS mounts exist ssh -i ~/.ssh/id_ed25519-ansible chris@dokploy.voidlabs.cc &quot;mount | grep nfs&quot; # Mount if missing ssh -i ~/.ssh/id_ed25519-ansible chris@dokploy.voidlabs.cc &quot; sudo mount -t nfs 192.168.50.2:/srv/fast-nfs /mnt/fast-nfs -o rw,soft,intr,timeo=30 sudo mount -t nfs 192.168.50.2:/srv/slow-nfs /mnt/slow-nfs -o rw,soft,intr,timeo=30 &quot; NFS mounts should be persistent in /etc/fstab : 192.168.50.2:/srv/fast-nfs /mnt/fast-nfs nfs rw,soft,intr,timeo=30 0 0 192.168.50.2:/srv/slow-nfs /mnt/slow-nfs nfs rw,soft,intr,timeo=30 0 0 ## Existing Buckets Existing Buckets Bucket Purpose Access Keys efta-images EFTA/DOJ document images docviewer (dev), docviewer-prod (production) Key Details Key Name Key ID Buckets Environment docviewer GKfdbcb1fcd0f2f1e7c36479e8 efta-images, docviewer-images Development docviewer-prod GK812ac25f8b505735494080f2 efta-images Production (Dokploy)"
    },
    {
      "id": "infrastructure/proxmox-gpu-passthrough-on-amd-kernel-68-iommu-issue",
      "title": "Proxmox GPU Passthrough on AMD - Kernel 6.8+ IOMMU Issue",
      "tags": "proxmox gpu-passthrough amd troubleshooting",
      "content": "Proxmox GPU Passthrough on AMD - Kernel 6.8+ IOMMU Issue Problem GPU passthrough fails on AMD platforms with kernel 6.8.4-3+ with error: vfio-pci 0000:01:00.0: Firmware has requested this device have a 1:1 IOMMU mapping, rejecting configuring the device without a 1:1 mapping. Contact your platform vendor. Root Cause Kernel 6.8.4-3 introduced stricter IOMMU enforcement that conflicts with newer NVIDIA GPUs (tested with RTX 4080 SUPER) on AMD platforms. Solution: Use Older Kernel Install and boot from kernel 6.5.x which predates the strict enforcement: # Install older kernel apt install proxmox-kernel-6.5.11-8-pve-signed # Find menu entry IDs grep -E &quot;menuentry|submenu&quot; /boot/grub/grub.cfg # Set as default (use actual IDs from above) sed -i 's/^GRUB_DEFAULT=.*/GRUB_DEFAULT=&quot;gnulinux-advanced-..&gt;gnulinux-6.5.11-8-pve-advanced-..&quot;/' /etc/default/grub grub-mkconfig -o /boot/grub/grub.cfg reboot BIOS Settings That Don't Help These were tested and did NOT fix the issue on kernel 6.8+: Disabling Resizable BAR Disabling Above 4G Decoding Adding pci=realloc kernel parameter Adding iommu.strict=0 kernel parameter Hardware Tested Motherboard : MSI Pro B650-VC WiFi III CPU : AMD (Ryzen series) GPU : NVIDIA RTX 4080 SUPER Proxmox : 8.x with kernel 6.8.12-17-pve (failed), 6.5.11-8-pve (works) Related Links Proxmox Forum: GPU Passthrough Issues Since 8.2"
    },
    {
      "id": "infrastructure/voidlabs-networking-and-dns",
      "title": "Voidlabs Networking and DNS",
      "tags": "networking dns dhcp caddy opnsense infrastructure",
      "content": "Voidlabs Networking and DNS The voidlabs network is managed through OPNsense with automated DHCP, DNS, and reverse proxy configuration. DNS Architecture External: *.voidlabs.cc → Caddy (192.168.51.80) → Backend services Internal: *.voidlabs.local → Unbound on OPNsense Components AdGuard Home (192.168.51.53) - DNS blocking and rewrites Unbound (on OPNsense) - Internal DNS resolution Caddy (on OPNsense) - Reverse proxy with automatic TLS Service Domain Configuration Domains are defined in infrastructure.yml : service_domains: # Route through Caddy reverse proxy - domain: app.voidlabs.cc target: caddy # Direct to host IP - domain: admin.voidlabs.cc target: host # Reference another managed host - domain: api.voidlabs.cc target: other_host # Literal IP override - domain: custom.voidlabs.cc target_ip: 192.168.51.100 DHCP Configuration Static DHCP reservations are managed via Ansible: managed_hosts: myapp: reserved_ip: 192.168.51.XX dhcp: true # or &quot;net0&quot; | &quot;net1&quot; for multi-interface VMs mac: auto # Extracted from Terraform state The opnsense_dhcp.yml playbook: Reads infrastructure.yml Extracts MACs from Terraform state Creates static DHCP reservations via OPNsense API Creates Unbound DNS host overrides Removes orphaned &quot;Ansible managed&quot; entries Caddy Reverse Proxy Managed via OPNsense API integration: managed_hosts: myapp: caddy: - name: myapp.voidlabs.cc port: 8080 skip_tls_verify: false # For self-signed backend certs TLS Configuration Wildcard subjects: *.voidlabs.cc , *.pt.voidlabs.cc DNS provider: Cloudflare (for ACME DNS challenges) Automatic certificate renewal Custom Ansible Filters # opnsense_filters.py terraform_mac_lookup(state, hostname, interface) # Extract MACs split_fqdn(domain) # Parse for Unbound resolve_dns_target(hosts, target, ...) # DNS resolution Related Playbooks opnsense_dhcp.yml - Sync DHCP and DNS opnsense_caddy.yml - Sync reverse proxy config adguard_dns.yml - Configure AdGuard rewrites Related Entries Voidlabs Infrastructure Overview Voidlabs Provisioning Workflow"
    },
    {
      "id": "infrastructure/dokploy/dokploy-vm-crash-recovery-and-traefik-troubleshooting",
      "title": "Dokploy VM Crash Recovery and Traefik Troubleshooting",
      "tags": "dokploy traefik vm-crash troubleshooting docker nfs systemd proxmox 503-error connection-refused container-not-running restart-policy persistent-logging journald boot-loop recovery infrastructure vm-restart",
      "content": "Dokploy VM Crash Recovery and Traefik Troubleshooting Problem Summary After VM crashes or restarts, dokploy.voidlabs.cc becomes unreachable with &quot;connection refused&quot; on ports 80/443, even though the VM is running and pingable. Symptoms: VM responds to ping but HTTPS fails with &quot;connection refused&quot; Port 3000 (Dokploy internal) still accessible directly Multiple quick reboots in boot history systemctl --failed shows mount failures Root Cause Analysis Primary Issue: Traefik Not Auto-Restarting The dokploy-traefik container had restart policy set to no , meaning it doesn't restart after VM reboot: # Check restart policy docker inspect dokploy-traefik --format '{{.HostConfig.RestartPolicy.Name}}' # Output: no &lt;-- This is the problem Secondary Issue: Stale NFS Mount A stale /srv/shares entry in /etc/fstab was causing systemd mount failures: mount.nfs: access denied by server while mounting 192.168.50.2:/srv/shares This mount was never exported on quasar - only /srv/fast-nfs and /srv/slow-nfs exist. Diagnostic Commands Check if Traefik is running # Via PVE guest agent (when SSH unavailable) ssh root@192.168.50.2 &quot;qm guest exec 118 -- docker ps --filter name=traefik&quot; # Direct (when SSH works) ssh root@dokploy.voidlabs.cc &quot;docker ps --filter name=traefik&quot; Check port connectivity nc -zv 192.168.51.87 22 80 443 3000 # Expected: 22 open, 80/443 open (if Traefik running), 3000 open Check boot history for crash patterns ssh root@192.168.50.2 &quot;qm guest exec 118 -- journalctl --list-boots&quot; Check failed systemd units ssh root@192.168.50.2 &quot;qm guest exec 118 -- systemctl --failed&quot; Resolution Steps 1. Start Traefik Container ssh root@192.168.50.2 &quot;qm guest exec 118 -- docker start dokploy-traefik&quot; 2. Set Traefik Restart Policy ssh root@192.168.50.2 &quot;qm guest exec 118 -- docker update --restart=unless-stopped dokploy-traefik&quot; 3. Remove Stale NFS Mount (if applicable) # Check fstab for stale mounts ssh root@192.168.50.2 &quot;qm guest exec 118 -- cat /etc/fstab&quot; # Remove stale entry ssh root@192.168.50.2 &quot;qm guest exec 118 -- sed -i '/192.168.50.2:\\\\/srv\\\\/shares/d' /etc/fstab&quot; # Reload systemd and clear failed units ssh root@192.168.50.2 &quot;qm guest exec 118 -- systemctl daemon-reload&quot; ssh root@192.168.50.2 &quot;qm guest exec 118 -- systemctl reset-failed&quot; 4. Start Other Stopped Containers Docker Compose containers don't auto-restart like Swarm services: ssh root@192.168.50.2 &quot;qm guest exec 118 -- docker start &lt;container-name&gt;&quot; Logging Configuration Persistent journald logging was enabled to survive reboots: # Create config mkdir -p /etc/systemd/journald.conf.d cat &gt; /etc/systemd/journald.conf.d/persistent.conf &lt;&lt; 'EOF' [Journal] Storage=persistent SystemMaxUse=500M EOF # Restart journald systemctl restart systemd-journald To check crash logs after next incident: # Check previous boot logs journalctl -b -1 --no-pager | tail -100 # Check all error-level messages from last boot journalctl -b -1 -p err --no-pager Prevention Traefik restart policy - Now set to unless-stopped Clean fstab - Removed stale mounts that don't have corresponding NFS exports Persistent logging - Journal survives reboots for post-mortem analysis VM Access Methods When direct SSH fails, use PVE guest agent: # Run command via guest agent ssh root@192.168.50.2 &quot;qm guest exec 118 -- &lt;command&gt;&quot; # Check VM status ssh root@192.168.50.2 &quot;qm list | grep dokploy&quot; Related Dokploy Network Recovery After VM Restart - For 504 errors after restart Dokploy Container Management - Container naming and operations Proxmox VM Filesystem Recovery - For filesystem corruption issues Dokploy Compose Deployment Lessons - Networking gotchas Search Keywords dokploy down, traefik not running, connection refused 443, connection refused 80, dokploy unreachable, vm crash, boot loop, container not starting, docker restart policy, nfs mount failed, access denied by server, systemd failed units, journald persistent, qm guest exec, proxmox vm troubleshooting"
    },
    {
      "id": "infrastructure/dokploy/dokploy-service-naming-and-database-url",
      "title": "Dokploy Service Naming and DATABASE_URL",
      "tags": "dokploy database docker-swarm configuration troubleshooting postgresql database-url service-discovery dns connection-error host-not-found name-resolution",
      "content": "Dokploy Service Naming and DATABASE_URL Problem When connecting services in Dokploy (e.g., backend to database), using the wrong hostname in DATABASE_URL causes connection failures: failed to resolve host 'epstein-db': Name or service not known Root Cause Dokploy creates unique service names by appending a random suffix to avoid conflicts: Compose defines: db or epstein-db Dokploy creates: docviewer-epsteindb-uouojr The Docker DNS name is the Dokploy-generated name , not the compose service name. Solution Find the correct service name # List all swarm services docker service ls | grep -i &lt;keyword&gt; # Example output: # m4mlqzp72zxh docviewer-epsteindb-uouojr replicated 1/1 pgvector/pgvector:pg16 Update DATABASE_URL Use the full Dokploy service name: # Wrong DATABASE_URL=postgresql+psycopg://user:pass@epstein-db:5432/dbname # Correct DATABASE_URL=postgresql+psycopg://user:pass@docviewer-epsteindb-uouojr:5432/dbname Update via Docker Swarm docker service update --env-add 'DATABASE_URL=postgresql+psycopg://user:pass@docviewer-epsteindb-uouojr:5432/dbname' &lt;service-name&gt; Best Practice When setting up DATABASE_URL in Dokploy environment variables, always verify the actual service name first with docker service ls . Related dokploy-deployment-guide dokploy-network-recovery ## Search Keywords host not found, name or service not known, connection refused, database connection, postgres connection, service name, docker dns, swarm service name"
    },
    {
      "id": "infrastructure/dokploy/dokploy-compose-deployment-lessons",
      "title": "Dokploy Compose Deployment Lessons",
      "tags": "dokploy networking compose traefik isolated-deployments troubleshooting reference",
      "content": "Dokploy Compose Networking - Definitive Guide Last verified: December 2025 Source code reference: /srv/fast/code/dokploy-for-reference/dokploy Docs reference: /srv/fast/code/dokploy-for-reference/website/apps/docs/content/docs/core/docker-compose/ TL;DR - The Rule If your compose has inter-service communication (frontend→backend, app→db), you MUST either: ✅ Enable Isolated Deployments in Dokploy UI (recommended), OR ✅ Manually add ALL services to dokploy-network in your compose file If you don't do one of these, services can't talk to each other. How Dokploy Networking Actually Works The Core Mechanism When you add a domain to a compose service in Dokploy: WITHOUT Isolated Deployments ( isolatedDeployment: false ): Dokploy adds ONLY the service with the domain to dokploy-network Other services stay on compose's auto-created default network Result: Service with domain can't reach other services! WITH Isolated Deployments ( isolatedDeployment: true ): Dokploy creates a network named after your appName Dokploy connects Traefik TO that network All services communicate via compose's network Result: Everything works! Source Code Proof File: dokploy/packages/server/src/utils/builders/compose.ts // Lines 56-58: What Isolated Deployments does ${compose.isolatedDeployment ? `docker network create --attachable ${compose.appName}` : &quot;&quot;} // After deploy, connects Traefik to the compose network: ${compose.isolatedDeployment ? `docker network connect ${compose.appName} $(docker ps --filter &quot;name=dokploy-traefik&quot; -q)` : &quot;&quot;} File: dokploy/packages/server/src/utils/docker/domain.ts // Lines 217-222: Without isolation, only domain service gets network if (!compose.isolatedDeployment) { result.services[serviceName].networks = addDokployNetworkToService( result.services[serviceName].networks, ); } Documentation Quote From website/apps/docs/content/docs/core/docker-compose/domains.mdx (line 88): &quot;If you're not using Isolated Deployments, Dokploy will add the dokploy-network to the service you selected, however you need to add dokploy-network to the other services to maintain connectivity .&quot; How to Check Current Settings # Get compose isolation setting curl -s -H &quot;x-api-key: $DOKPLOY_API_KEY&quot; \\ &quot;https://dokploy.voidlabs.cc/api/compose.one?composeId=&lt;ID&gt;&quot; | \\ jq '{appName, isolatedDeployment, randomize}' How to Enable Isolated Deployments Via API: curl -s -X POST \\ -H &quot;x-api-key: $DOKPLOY_API_KEY&quot; \\ -H &quot;Content-Type: application/json&quot; \\ -d '{&quot;composeId&quot;: &quot;&lt;ID&gt;&quot;, &quot;isolatedDeployment&quot;: true}' \\ https://dokploy.voidlabs.cc/api/compose.update Via UI: Go to compose → General tab Toggle &quot;Isolated Deployments&quot; ON Redeploy Network Debugging Commands # Check what networks a container is on docker inspect &lt;container&gt; --format '{{json .NetworkSettings.Networks}}' | jq -r 'keys[]' # Check what networks Traefik is connected to docker inspect dokploy-traefik --format '{{json .NetworkSettings.Networks}}' | jq -r 'keys[]' # Test DNS resolution from inside a container docker exec &lt;container&gt; nslookup &lt;service-name&gt; # List all containers with their networks docker ps --format 'table {{.Names}}\\t{{.Networks}}' Common Mistakes We Made Mistake 1: Blaming &quot;networking&quot; generically Reality: The issue was always that Dokploy only adds the domain service to dokploy-network. Mistake 2: Creating custom networks as workaround Reality: Works, but the proper fix is enabling Isolated Deployments. Mistake 3: Looking at other deployed apps as &quot;correct&quot; examples Reality: Other apps (Phase, etc.) may also be misconfigured. Always compare against Dokploy source. Reference Files What Path Compose builder (isolation logic) dokploy/packages/server/src/utils/builders/compose.ts Domain/network injection dokploy/packages/server/src/utils/docker/domain.ts Traefik setup dokploy/packages/server/src/setup/traefik-setup.ts Docs: Compose utilities website/apps/docs/content/docs/core/docker-compose/utilities.mdx Docs: Compose domains website/apps/docs/content/docs/core/docker-compose/domains.mdx Related dokploy-deployment-guide - CLI and API reference docviewer-deployment-architecture - DocViewer specific setup"
    },
    {
      "id": "infrastructure/dokploy/dokploy-container-management",
      "title": "Dokploy Container Management",
      "tags": "dokploy docker containers debugging infrastructure",
      "content": "Dokploy Container Management Overview Dokploy uses Docker Swarm to manage containers. Understanding the container naming conventions and management tools helps with debugging and operations. Container Naming Conventions Dokploy generates container names following this pattern: &lt;project&gt;-&lt;service&gt;-&lt;random-suffix&gt; Examples: docviewer-docviewer-pvxtnc - Application container compose-parse-auxiliary-port-cgcdph - Auto-generated compose name voidlabssharedservices-garagewithui-vo0uq2-garage-fast-1 - Compose service Finding Containers Using the Dokploy CLI # List containers matching an app name dokploy docker containers &lt;app-name&gt; # Example output: # ● docviewer-backend-abc123 # State: running (Up 2 hours) # Image: ghcr.io/user/docviewer:latest # ID: a1b2c3d4e5f6 # Ports: 8000 Using the API curl -s -H &quot;x-api-key: $DOKPLOY_API_KEY&quot; \\ &quot;https://dokploy.voidlabs.cc/api/docker.getContainersByAppNameMatch?appName=docviewer&quot; Direct Docker Commands (on server) # SSH to dokploy server ssh dokploy.voidlabs.cc # List all containers docker ps # Filter by name docker ps --filter &quot;name=docviewer&quot; # Check swarm services docker service ls docker service ps &lt;service-name&gt; --no-trunc Common Container Operations Restart a Container # Via CLI dokploy docker restart &lt;container-id&gt; # Via API curl -X POST -H &quot;x-api-key: $DOKPLOY_API_KEY&quot; \\ -H &quot;Content-Type: application/json&quot; \\ -d '{&quot;containerId&quot;:&quot;&lt;container-id&gt;&quot;}' \\ https://dokploy.voidlabs.cc/api/docker.restartContainer Get Container Config dokploy docker config &lt;container-id&gt; View Container Logs Container logs must be accessed directly on the server: ssh dokploy.voidlabs.cc &quot;docker logs &lt;container-id&gt; --tail 100&quot; Debugging Non-Running Services When an application shows &quot;running&quot; in Dokploy but no container exists: 1. Check Swarm Service Status ssh dokploy.voidlabs.cc &quot;docker service ps &lt;app-name&gt; --no-trunc&quot; This shows why containers failed to start. 2. Common Failure Reasons Error Cause Solution bind source path does not exist Missing mount directory Create directory or remove mount port already allocated Port conflict Change port or stop conflicting service image not found Registry auth or missing image Check registry config, rebuild OOM killed Out of memory Increase memory limit 3. Check Deployment Logs # Get log path from deployment history dokploy app logs &lt;app-name&gt; # Read the log file ssh dokploy.voidlabs.cc &quot;cat /etc/dokploy/logs/&lt;app-name&gt;/&lt;log-file&gt;.log&quot; Known Container Names For admin operations on shared services: Service Container Name Garage Fast voidlabssharedservices-garagewithui-vo0uq2-garage-fast-1 Garage Slow voidlabssharedservices-garagewithui-vo0uq2-garage-slow-1 Garage WebUI Fast voidlabssharedservices-garagewithui-vo0uq2-garage-webui-fast-1 Garage WebUI Slow voidlabssharedservices-garagewithui-vo0uq2-garage-webui-slow-1 Related Dokploy Deployment Guide - CLI and API reference Garage S3 Object Storage - Garage container operations Voidlabs Infrastructure Overview - Server details"
    },
    {
      "id": "infrastructure/dokploy/deployment-guide",
      "title": "Dokploy Deployment Guide",
      "tags": "dokploy deployment api docker infrastructure",
      "content": "Dokploy Deployment Guide Overview Dokploy is our self-hosted PaaS at dokploy.voidlabs.cc for deploying Docker applications and compose stacks. Authentication API Key Stored in Phase: DOKPLOY_API_KEY in the docviewer app (Development environment) Use header: x-api-key: &lt;token&gt; (NOT Authorization: Bearer ) # Get API key from Phase cd /srv/fast/code/epstein &amp;&amp; phase secrets get DOKPLOY_API_KEY --plain # Set in environment export DOKPLOY_API_KEY=&quot;...&quot; Dokploy CLI (Recommended) A CLI tool is available at /srv/fast/code/dokploy-python-client . This is the recommended way for AI agents to interact with Dokploy. Installation cd /srv/fast/code/dokploy-python-client uv sync # Set API key export DOKPLOY_API_KEY=&quot;...&quot; # Test connection dokploy Complete Command Reference Application Commands ( dokploy app ) dokploy app list [-v] # List all applications dokploy app status &lt;id-or-name&gt; # Get application details dokploy app redeploy &lt;id-or-name&gt; [-t &quot;reason&quot;] # Trigger redeploy dokploy app logs &lt;id-or-name&gt; [-n 5] # Show deployment history dokploy app start &lt;id-or-name&gt; # Start application dokploy app stop &lt;id-or-name&gt; # Stop application Compose Commands ( dokploy compose ) dokploy compose list [-v] # List all compose stacks dokploy compose info &lt;id-or-name&gt; # Get compose details dokploy compose deploy &lt;id-or-name&gt; # Initial deployment dokploy compose redeploy &lt;id-or-name&gt; [-t &quot;reason&quot;] # Redeploy dokploy compose start &lt;id-or-name&gt; # Start compose dokploy compose stop &lt;id-or-name&gt; # Stop compose dokploy compose logs &lt;id-or-name&gt; [-n 5] # Show deployment history dokploy compose services &lt;id-or-name&gt; # List services in stack Domain Commands ( dokploy domains ) dokploy domains list &lt;compose&gt; # List domains for compose dokploy domains add &lt;compose&gt; &lt;host&gt; &lt;service&gt; &lt;port&gt; [--path /api] [--strip-path] dokploy domains update &lt;domain_id&gt; [--host x] [--port y] [--service z] dokploy domains rm &lt;domain_id&gt; # Delete domain Docker Commands ( dokploy docker ) dokploy docker containers &lt;app-name&gt; # List containers (debugging) dokploy docker restart &lt;container-id&gt; # Restart container dokploy docker config &lt;container-id&gt; # Get container config Project Commands ( dokploy projects ) dokploy projects list # List all projects Finding Resource IDs Run dokploy without arguments to discover IDs: docviewer-dev (NcwCKurRv7tXNlctzRyJR) Applications: ● docviewer-backend (Ufjj6UaPE9tC4S2DYsVYh) Composes: ● postgres-dev (wcaBZPm67rCP8JCRADH2U) ● qdrant-dev (beeo6OVKAACLHeoLH5B4Q) Total: 1 application(s), 2 compose(s) Partial name matching is supported: dokploy app status docviewer # Matches &quot;docviewer-backend&quot; dokploy compose redeploy postgres # Matches &quot;postgres-dev&quot; Common Workflows Redeploy an Application dokploy app list dokploy app redeploy docviewer-backend -t &quot;Updated config&quot; dokploy app logs docviewer-backend Debug a Non-Running Service # List containers to see what's actually running dokploy docker containers docviewer # If no containers found, check status dokploy app status docviewer dokploy compose info postgres-dev # Check deployment logs for errors dokploy app logs docviewer -n 10 Add a Domain to a Compose dokploy compose list dokploy domains add postgres-dev db.example.com postgres 5432 dokploy domains list postgres-dev Raw API Operations (curl) For operations not covered by the CLI: List All Projects and Composes curl -s -H &quot;x-api-key: $DOKPLOY_API_KEY&quot; \\ https://dokploy.voidlabs.cc/api/project.all Get Application Details curl -s -H &quot;x-api-key: $DOKPLOY_API_KEY&quot; \\ &quot;https://dokploy.voidlabs.cc/api/application.one?applicationId=&lt;APP_ID&gt;&quot; Trigger Application Redeploy curl -s -X POST \\ -H &quot;x-api-key: $DOKPLOY_API_KEY&quot; \\ -H &quot;Content-Type: application/json&quot; \\ -d '{&quot;applicationId&quot;:&quot;&lt;APP_ID&gt;&quot;,&quot;title&quot;:&quot;Deployment reason&quot;}' \\ https://dokploy.voidlabs.cc/api/application.redeploy Get Deployment History curl -s -H &quot;x-api-key: $DOKPLOY_API_KEY&quot; \\ &quot;https://dokploy.voidlabs.cc/api/deployment.all?applicationId=&lt;APP_ID&gt;&quot; Get Running Containers curl -s -H &quot;x-api-key: $DOKPLOY_API_KEY&quot; \\ &quot;https://dokploy.voidlabs.cc/api/docker.getContainersByAppNameMatch?appName=&lt;NAME&gt;&quot; Viewing Build Logs Logs are stored on the Dokploy server. Get the log path from dokploy app logs : ssh dokploy.voidlabs.cc &quot;cat /etc/dokploy/logs/&lt;app-name&gt;/&lt;log-file&gt;.log&quot; Python API Usage For programmatic access beyond the CLI: from dokploy_api_client import AuthenticatedClient from dokploy_api_client.api.application import application_redeploy from dokploy_api_client.models import ApplicationRedeployBody import os # IMPORTANT: Use x-api-key header, not Bearer token client = AuthenticatedClient( base_url=&quot;https://dokploy.voidlabs.cc/api&quot;, token=os.environ[&quot;DOKPLOY_API_KEY&quot;], prefix=&quot;&quot;, # No &quot;Bearer&quot; prefix auth_header_name=&quot;x-api-key&quot;, # Custom header name ) with client as c: body = ApplicationRedeployBody(application_id=&quot;Ufjj6UaPE9tC4S2DYsVYh&quot;) result = application_redeploy.sync_detailed(client=c, body=body) print(f&quot;Status: {result.status_code}&quot;) API Modules Reference Module Purpose api.project Project CRUD, listing api.application App deploy, redeploy, start, stop, env vars api.compose Compose deploy, redeploy, services api.deployment Deployment history, logs api.docker Container management api.domain Domain/routing configuration Environment Variables Variable Default Description DOKPLOY_API_KEY (required) API authentication key DOKPLOY_URL https://dokploy.voidlabs.cc/api API base URL Troubleshooting &quot;DOKPLOY_API_KEY not found&quot; Set the environment variable or create a .env file with DOKPLOY_API_KEY=... 401 Unauthorized Verify the API key is correct The client uses x-api-key header (not Bearer token) &quot;Multiple matches&quot; error When partial name matching finds multiple results, use the full ID instead. Container Fails to Start If the application shows &quot;running&quot; status in Dokploy but no container exists: # Debug with CLI dokploy docker containers &lt;app-name&gt; # Or check Docker Swarm directly docker service ps &lt;app-name&gt; --no-trunc Common issues: Missing bind mount path : Container configured with a path that doesn't exist on the server Port mismatch : Domain configured with wrong port Server Unreachable If dokploy.voidlabs.cc is unreachable: Check if it's a network issue vs server down The server IP is 192.168.51.87 Contact infrastructure team if persistent Related dokploy-python-cli-reference - Complete CLI command reference Voidlabs Common Patterns - Deployment patterns DocViewer Project - Main docviewer documentation ## SSH Access SSH Access To SSH to the Dokploy VM: # Use the ansible key with chris user ssh -i ~/.ssh/id_ed25519-ansible chris@dokploy.voidlabs.cc # Or with the ansible key loaded ssh chris@dokploy.voidlabs.cc Key points: IP: 192.168.51.87 User: chris (not root) Key: ~/.ssh/id_ed25519-ansible Direct Container Access # Run docker commands ssh -i ~/.ssh/id_ed25519-ansible chris@dokploy.voidlabs.cc &quot;docker ps&quot; # Check logs ssh -i ~/.ssh/id_ed25519-ansible chris@dokploy.voidlabs.cc &quot;docker logs &lt;container&gt;&quot; # Exec into container ssh -i ~/.ssh/id_ed25519-ansible chris@dokploy.voidlabs.cc &quot;docker exec -it &lt;container&gt; sh&quot; ### Common Workflows Common Workflows Deploy vs Redeploy CRITICAL DIFFERENCE: Command Git Pull? Container Recreate? Use When deploy ✅ Yes ✅ Yes Code changed in git redeploy ❌ No ⚠️ Only if config changed Just restart containers If you pushed code changes to git, you MUST use deploy , not redeploy ! # After pushing code changes to git dokploy compose deploy &lt;name&gt; # Pulls from git, recreates containers # Just restart existing containers (no git pull) dokploy compose redeploy &lt;name&gt; # Only restarts, does NOT pull from git Update Code and Redeploy # 1. Make changes to compose file in repo cd /srv/fast/code/voidlabs-ansible vim dokploy/garage/docker-compose.yml # 2. Commit and push git add . &amp;&amp; git commit -m &quot;fix: update config&quot; &amp;&amp; git push # 3. Deploy (NOT redeploy) to pull changes dokploy compose deploy garage-with-ui WARNING: Never edit files directly in /etc/dokploy/compose/*/code/ - this bypasses Dokploy's workflow and will cause issues on next deploy. ## Raw API Operations (curl) Raw API Operations (curl) For operations not covered by the CLI: Create Compose curl -s -X POST \\ -H &quot;x-api-key: $DOKPLOY_API_KEY&quot; \\ -H &quot;Content-Type: application/json&quot; \\ -d '{ &quot;name&quot;: &quot;my-compose&quot;, &quot;projectId&quot;: &quot;&lt;PROJECT_ID&gt;&quot;, &quot;environmentId&quot;: &quot;&lt;ENVIRONMENT_ID&gt;&quot;, &quot;description&quot;: &quot;My compose stack&quot; }' \\ https://dokploy.voidlabs.cc/api/compose.create Configure Compose GitHub Source curl -s -X POST \\ -H &quot;x-api-key: $DOKPLOY_API_KEY&quot; \\ -H &quot;Content-Type: application/json&quot; \\ -d '{ &quot;composeId&quot;: &quot;&lt;COMPOSE_ID&gt;&quot;, &quot;sourceType&quot;: &quot;github&quot;, &quot;repository&quot;: &quot;repo-name&quot;, &quot;owner&quot;: &quot;owner&quot;, &quot;branch&quot;: &quot;main&quot;, &quot;composePath&quot;: &quot;./docker-compose.prod.yml&quot;, &quot;githubId&quot;: &quot;&lt;GITHUB_ID&gt;&quot; }' \\ https://dokploy.voidlabs.cc/api/compose.update Set Compose Environment Variables curl -s -X POST \\ -H &quot;x-api-key: $DOKPLOY_API_KEY&quot; \\ -H &quot;Content-Type: application/json&quot; \\ -d '{ &quot;composeId&quot;: &quot;&lt;COMPOSE_ID&gt;&quot;, &quot;env&quot;: &quot;KEY1=value1\\nKEY2=value2&quot; }' \\ https://dokploy.voidlabs.cc/api/compose.update Deploy Compose (pulls fr"
    },
    {
      "id": "infrastructure/dokploy/docker-cp-permission-patterns-in-dokploy",
      "title": "Docker CP Permission Patterns in Dokploy",
      "tags": "dokploy docker permissions troubleshooting containers",
      "content": "Docker CP Permission Patterns in Dokploy When copying files into running Dokploy containers, permission issues commonly occur. The Problem Files copied via docker cp retain the host user's UID: -rw-r--r-- 1 1001 1001 110561 Dec 22 22:21 VOL00001.DAT But the container runs as a different user (e.g., appuser with UID 1000), causing: PermissionError: [Errno 13] Permission denied: '/tmp/doj-fixed/VOL00001.DAT' Solution Pattern # 1. Copy files to container docker cp /local/path container:/tmp/destination # 2. Fix ownership (requires root) docker exec --user root container chown -R appuser:appuser /tmp/destination # 3. Verify permissions docker exec container ls -la /tmp/destination Alternative: Pre-fix on Host If you know the target UID before copying: # On host, before docker cp sudo chown -R 1000:1000 /local/path docker cp /local/path container:/tmp/destination DocViewer Specifics DocViewer backend container runs as appuser (UID 1000): docker exec docviewer-prod-xxx-backend-1 id # uid=1000(appuser) gid=1000(appuser) groups=1000(appuser) Related dokploy-deployment-guide - General Dokploy patterns docviewer-deployment-architecture - DocViewer container structure"
    },
    {
      "id": "infrastructure/dokploy/dokploy-network-recovery-after-vm-restart",
      "title": "Dokploy Network Recovery After VM Restart",
      "tags": "dokploy traefik networking troubleshooting docker recovery 504-error gateway-timeout isolated-network vm-restart docker-compose bad-gateway",
      "content": "Dokploy Network Recovery After VM Restart Problem After a VM restart, crash, or filesystem recovery, Docker Compose services deployed with Dokploy's isolated network feature may lose their Traefik routing. Symptoms include: 504 Gateway Timeout on previously working domains Traefik labels exist on containers but routing doesn't work Containers are on isolated networks (e.g., voidlabssharedservices-phase-kdkgev ) separate from dokploy-network Root Cause From Dokploy docs : If using a custom installation with Traefik as a Docker service (rather than a standalone container), system restarts can cause services to &quot;lose their network references to Traefik.&quot; Dokploy's isolated deployments create app-specific networks and Traefik is supposed to connect to each. After restarts, these connections can break. Solution DO NOT manually connect networks with docker network connect . This is a band-aid that doesn't persist and can cause other issues. DO redeploy through Dokploy : Go to Dokploy UI → find the affected compose/application Click &quot;Redeploy&quot; This reconnects the isolated network to Traefik properly Prevention Use Dokploy's official installation with standalone Traefik container (not Docker Swarm service) After any VM recovery, check all isolated apps and redeploy if needed Related dokploy-deployment-guide Dokploy uses Docker labels for Traefik routing (e.g., traefik.http.routers.*.rule ) ## Search Keywords 504, bad gateway, gateway timeout, traefik not routing, container unreachable, isolated network, docker network connect, redeploy, vm crash, system restart"
    },
    {
      "id": "infrastructure/dokploy/dokploy-python-cli-reference",
      "title": "Dokploy Python CLI Reference",
      "tags": "dokploy cli python api deployment reference",
      "content": "Dokploy Python CLI Reference Complete reference for the Dokploy Python CLI at /srv/fast/code/dokploy-python-client . Quick Start cd /srv/fast/code/dokploy-python-client export DOKPLOY_API_KEY=&quot;...&quot; # Get from Phase uv run dokploy Application Commands ( dokploy app ) Listing &amp; Status dokploy app list [-v] # List all applications dokploy app status &lt;id-or-name&gt; # Get application details dokploy app logs &lt;id-or-name&gt; [-n] # Show deployment history Lifecycle dokploy app start &lt;id-or-name&gt; # Start application dokploy app stop &lt;id-or-name&gt; # Stop application dokploy app deploy &lt;id-or-name&gt; [-t &quot;reason&quot;] # Pull from git &amp; deploy dokploy app redeploy &lt;id-or-name&gt; [-t &quot;reason&quot;] # Restart containers (no git pull) CRITICAL: deploy pulls from git, redeploy does not. After pushing code, use deploy . CRUD dokploy app create &lt;name&gt; -p &lt;project&gt; [-d &quot;description&quot;] [-e env-name] dokploy app delete &lt;id-or-name&gt; [-f] Update Settings dokploy app update &lt;id-or-name&gt; [OPTIONS] Options: Option Description Example --name, -n Display name --name &quot;My App&quot; --description, -d Description --description &quot;Production API&quot; --branch, -b Git branch --branch main --dockerfile Dockerfile path --dockerfile ./Dockerfile.prod --docker-image Docker image --docker-image nginx:latest --command, -c Run command --command &quot;npm start&quot; --replicas, -r Replica count --replicas 3 --auto-deploy/--no-auto-deploy Auto deploy on push --auto-deploy --memory-limit Memory limit --memory-limit 1g --cpu-limit CPU limit --cpu-limit 0.5 --build-path Build path in repo --build-path ./backend Examples: # Update git branch dokploy app update my-app --branch main # Configure build settings dokploy app update my-app --dockerfile ./Dockerfile --build-path ./backend # Scale and set resource limits dokploy app update my-app --replicas 3 --memory-limit 1g --cpu-limit 0.5 # Enable auto-deploy dokploy app update my-app --auto-deploy Environment Variables ( dokploy app env ) dokploy app env list &lt;id-or-name&gt; # List env vars dokploy app env set &lt;id-or-name&gt; KEY=value [KEY2=value2...] # Set (merges) dokploy app env set &lt;id-or-name&gt; -r KEY=val # Replace all vars dokploy app env unset &lt;id-or-name&gt; KEY [KEY2...] # Remove env vars Examples: # View current environment dokploy app env list my-app # Add/update variables (preserves existing) dokploy app env set my-app DATABASE_URL=postgres://... REDIS_URL=redis://... # Replace ALL variables dokploy app env set my-app --replace NEW_VAR=value # Remove variables dokploy app env unset my-app OLD_VAR DEPRECATED_VAR Compose Commands ( dokploy compose ) Listing &amp; Info dokploy compose list [-v] # List all compose stacks dokploy compose info &lt;id-or-name&gt; # Get compose details dokploy compose services &lt;id-or-name&gt; # List services in stack dokploy compose logs &lt;id-or-name&gt; [-n] # Show deployment history Lifecycle dokploy compose start &lt;id-or-name&gt; # Start compose dokploy compose stop &lt;id-or-name&gt; # Stop compose dokploy compose deploy &lt;id-or-name&gt; # Pull from git &amp; deploy dokploy compose redeploy &lt;id-or-name&gt; [-t &quot;reason&quot;] # Restart (no git pull) CRUD dokploy compose create &lt;name&gt; -p &lt;project&gt; [-d &quot;description&quot;] dokploy compose update &lt;id-or-name&gt; [OPTIONS] dokploy compose delete &lt;id-or-name&gt; [-f] Environment Variables ( dokploy compose env ) dokploy compose env list &lt;id-or-name&gt; dokploy compose env set &lt;id-or-name&gt; KEY=value dokploy compose env set &lt;id-or-name&gt; -r KEY=val # Replace all dokploy compose env unset &lt;id-or-name&gt; KEY Postgres Commands ( dokploy postgres ) dokploy postgres list # List all Postgres instances dokploy postgres status &lt;id&gt; # Get Postgres details dokploy postgres start &lt;id&gt; # Start instance dokploy postgres stop &lt;id&gt; # Stop instance dokploy postgres create &lt;name&gt; -p &lt;project&gt; [--database-name db] [--database-user user] dokploy postgres delete &lt;id&gt; [-f] Domain Commands ( dokploy domains ) dokploy domains list &lt;compose&gt; dokploy domains add &lt;compose&gt; &lt;host&gt; &lt;service&gt; &lt;port&gt; [--path /api] [--strip-path] dokploy domains update &lt;domain_id&gt; [--host x] [--port y] [--service z] dokploy domains rm &lt;domain_id&gt; Docker Commands ( dokploy docker ) For debugging container issues: dokploy docker containers &lt;app-name&gt; # List containers dokploy docker restart &lt;container-id&gt; # Restart container dokploy docker config &lt;container-id&gt; # Get container config Project Commands ( dokploy projects ) Project Commands ( dokploy projects ) Listing &amp; Info dokploy projects list [-v] # List all projects dokploy projects info &lt;id-or-name&gt; # Get project details with environments CRUD dokploy projects create &lt;name&gt; [-d &quot;description&quot;] dokploy projects update &lt;id-or-name&gt; [--name new-name] [--description &quot;desc&quot;] dokploy projects delete &lt;id-or-name&gt; [-f] Examples: # Create a project dokploy projects create my-project -d &quot;My awesome project&quot; # Update project name dokploy projects update my-project --name new-project-name # Delete with confirmation skip dokploy projects delete old-project -f Environment Commands ( dokploy env ) Environments are namespaces within projects for organizing services (e.g., staging, production). Listing &amp; Info dokploy env list [-p project] [-v] # List all environments dokploy env info &lt;id-or-name&gt; [-p project] # Get environment details CRUD dokploy env create &lt;name&gt; -p &lt;project&gt; [-d &quot;description&quot;] dokploy env update &lt;id-or-name&gt; [--name new-name] [--description &quot;desc&quot;] [-p project] dokploy env delete &lt;id-or-name&gt; [-f] [-p project] Examples: # Create staging environment dokploy env create staging -p my-project -d &quot;Staging environment&quot; # List environments in a project dokploy env list -p my-project -v # Get environment info dokploy env info staging -p my-project # Delete environment dokploy env delete old-env -f Name Matching All commands support partial name matching : dokploy app status docviewer # Matches &quot;docviewer-backend&quot; dokploy compose redeploy postgres # Matches &quot;postgres-dev&quot; If multiple matches found, use the full ID instead. Configuration Variable Default Description DOKPLOY_API_KEY (required) API key from Phase DOKPLOY_URL https://dokploy.voidlabs.cc/api API base URL Python API Usage For programmatic access: from dokploy_api_client import AuthenticatedClient from dokploy_api_client.api.application import application_update from dokploy_api_client.models import ApplicationUpdateBody import os client = AuthenticatedClient( base_url=&quot;https://dokploy.voidlabs.cc/api&quot;, token=os.environ[&quot;DOKPLOY_API_KEY&quot;], prefix=&quot;&quot;, auth_header_name=&quot;x-api-key&quot;, ) with client as c: body = ApplicationUpdateBody( application_id=&quot;&lt;APP_ID&gt;&quot;, branch=&quot;main&quot;, auto_deploy=True, ) result = application_update.sync_detailed(client=c, body=body) print(f&quot;Status: {result.status_code}&quot;) Common Workflows Deploy Code Changes # After pushing to git dokploy app deploy my-app -t &quot;Deploy v1.2.0&quot; dokploy app logs my-app Configure New Application dokploy app create my-app -p my-project dokploy app update my-app --branch main --dockerfile ./Dockerfile dokploy app env set my-app DATABASE_URL=... API_KEY=... dokploy app deploy my-app Scale Application dokploy app update my-app --replicas 3 --memory-limit 2g dokploy app redeploy my-app -t &quot;Scale to 3 replicas&quot; Related dokploy-deployment-guide - General deployment guide with SSH access, troubleshooting Voidlabs Common Patterns - Infrastructure patterns"
    },
    {
      "id": "projects/docviewer/status-and-recovery-plan",
      "title": "DocViewer Project Status & Recovery Plan",
      "tags": "docviewer efta epstein deployment status",
      "content": "DocViewer Project Status &amp; Recovery Plan Overview DocViewer is a document analysis tool for investigative reporters to browse, search, and analyze EFTA (Epstein Files Transparency Act) documents. Current State (as of 2025-12-22) Current State (as of 2025-12-22) Infrastructure Status Component Status Details Dokploy Server ⚠️ UNSTABLE Proxmox container keeps crashing Backend API 🔄 CONFIGURED Image built, env fixed, waiting for server stability Frontend ⚠️ NOT DEPLOYED Code exists, needs deploy after backend is stable PostgreSQL ✅ CONFIGURED Service started, 1/1 replicas Garage S3 ⚠️ PARTIAL Bucket exists, only 2 files uploaded Qdrant Vector ⚠️ UNKNOWN Not tested Fixes Applied (2025-12-22) Domain port : 5001 → 8000 ✅ Mount directory : Created /srv/shares/epstein-documents ✅ DATABASE_URL : Fixed to use docviewer-epsteindb-uouojr ✅ Backend image : Rebuilt with uvicorn on port 8000 ✅ Database service : Started successfully ✅ Pending Investigate Proxmox container stability Verify backend responds after server is stable Deploy frontend Data Status Metric Current Expected Completion Datasets 5 7+ Partial Documents 6,274 ~4,085 new Some overlap Images (S3) 2 30,833 0% OCR Complete 1,454 30,833 4.7% Embeddings 0 6,274 0% Critical Issues 1. Backend Not Running docker-compose.yml:62 has wrong module path dokploy-stack.yml:48 references missing script Port mismatch between container and exposure 2. Frontend Not Deployed No production Dockerfile Bug in DocumentPage.tsx:163 - image URLs missing datasetId 3. Images Not in S3 Upload interrupted by VirtioFS errors Switched to NFS but full sync never completed Recovery Plan Phase 1: Fix Deployment (P0) Fix docker-compose.yml module path Fix dokploy-stack.yml port and migration Create frontend Dockerfile Fix image URL bug Phase 2: Projects Feature (Required for reporters) Database migration for projects table Backend API endpoints Frontend project selector Phase 3: Ingestion Pipeline Normalize directory structure Create automation script Ingest DOJ datasets 1-7 Phase 4: OCR (GPU on hyperion) Process ~4,085 documents Phase 5: Embeddings Generate vectors for semantic search Tracking Epic : epstein-rqh Backend Bug : epstein-crq Frontend Bug : epstein-qms S3 Sync : epstein-zxd OCR : epstein-tds Embeddings : epstein-k9i Related Documentation docviewer-architecture - System architecture garage-s3-object-storage - S3 storage configuration voidlabs-storage-architecture - Storage tiers"
    },
    {
      "id": "projects/docviewer/docviewer-production-database-access",
      "title": "DocViewer Production Database Access",
      "tags": "docviewer database production dokploy postgresql troubleshooting",
      "content": "DocViewer Production Database Access DocViewer has separate dev and production databases. CLI tools need different access patterns for each. Database Topology Environment Host Port Access Dev dokploy.voidlabs.cc 5433 External (postgres-dev) Production db (internal) 5432 Docker internal only Dev Database Access (External) From local machine or devcontainer: # Via psql PGPASSWORD=changeme_dev psql -h dokploy.voidlabs.cc -p 5433 -U epstein -d epstein # Via CLI tools source .env # Contains DATABASE_URL pointing to :5433 docviewer-ingest import-production --dataset-id UUID ... Production Database Access Production DB has no external port. Options: Option 1: Exec into Backend Container (Recommended) # Copy files to server first scp data.dat dokploy.voidlabs.cc:/tmp/ # Copy into container ssh dokploy.voidlabs.cc &quot;docker cp /tmp/data.dat docviewer-prod-xxx-backend-1:/tmp/&quot; # Fix permissions ssh dokploy.voidlabs.cc &quot;docker exec --user root docviewer-prod-xxx-backend-1 chown -R appuser:appuser /tmp/data.dat&quot; # Run CLI ssh dokploy.voidlabs.cc &quot;docker exec docviewer-prod-xxx-backend-1 docviewer-ingest import-production ...&quot; Option 2: Direct DB Access via Container ssh dokploy.voidlabs.cc &quot;docker exec docviewer-prod-xxx-db-1 psql -U epstein -d epstein -c 'SELECT ...'&quot; Option 3: Execute SQL File scp script.sql dokploy.voidlabs.cc:/tmp/ ssh dokploy.voidlabs.cc &quot;docker cp /tmp/script.sql docviewer-prod-xxx-db-1:/tmp/&quot; ssh dokploy.voidlabs.cc &quot;docker exec docviewer-prod-xxx-db-1 psql -U epstein -d epstein -f /tmp/script.sql&quot; Finding Container Names Container names include random suffixes: ssh dokploy.voidlabs.cc &quot;docker ps | grep docviewer&quot; # docviewer-prod-rwlm59-backend-1 # docviewer-prod-rwlm59-db-1 # docviewer-prod-rwlm59-frontend-1 Common Gotcha API creates data in production, CLI reads from dev! If you create datasets via https://docviewer.voidlabs.cc/api/... but run CLI locally, you'll get &quot;Dataset not found&quot; because CLI connects to dev DB. Related docviewer-deployment-architecture - Full architecture overview docker-cp-permission-patterns-in-dokploy - Permission fixes"
    },
    {
      "id": "projects/docviewer/dataset-ingestion-workflow",
      "title": "DocViewer Dataset Ingestion Workflow",
      "tags": "docviewer ingestion workflow datasets s3",
      "content": "DocViewer Dataset Ingestion Workflow Overview Standardized process to ingest new document datasets into DocViewer. Prerequisites Backend API running Garage S3 bucket accessible ( efta-images ) PostgreSQL database accessible Dataset files extracted to /srv/fast/datashare/efta/ Directory Structure Expected Layout /srv/fast/datashare/efta/doj-{N}/ ├── DATA/ │ ├── VOL000XX.DAT # Concordance metadata │ └── VOL000XX.OPT # Image path mappings └── IMAGES/ └── *.pdf # Document images DOJ Dataset Reference Dataset Slug PDFs Size 1 doj-1 3,142 2.5GB 2 doj-2 574 1.3GB 3 doj-3 67 1.2GB 4 doj-4 152 708MB 5 doj-5 120 125MB 6 doj-6 13 104MB 7 doj-7 17 195MB Ingestion Steps Step 1: Create Dataset via API curl -X POST https://docviewer.voidlabs.cc/api/v1/datasets/ \\ -H 'Content-Type: application/json' \\ -d '{ &quot;name&quot;: &quot;DOJ Dataset 1&quot;, &quot;slug&quot;: &quot;doj-1&quot;, &quot;description&quot;: &quot;DOJ EFTA Disclosure - Dataset 1 (3,142 documents)&quot; }' Save the returned UUID as DATASET_ID . Step 2: Upload Images to S3 aws s3 sync /srv/fast/datashare/efta/doj-1/IMAGES/ \\ s3://efta-images/doj-1/ \\ --endpoint-url=https://garage-fast.voidlabs.cc Step 3: Ingest Metadata docviewer-ingest import-production \\ --dataset-id=&quot;$DATASET_ID&quot; \\ --data-dir=/srv/fast/datashare/efta/doj-1 This imports: Document metadata from DAT file Image paths from OPT file Text content (if TEXT files present) Step 4: Verify # Check document count curl https://docviewer.voidlabs.cc/api/v1/datasets/$DATASET_ID # Check S3 file count aws s3 ls s3://efta-images/doj-1/ --recursive \\ --endpoint-url=https://garage-fast.voidlabs.cc | wc -l Post-Ingestion Queue OCR docviewer-ocr queue --dataset=doj-1 Generate Embeddings docviewer-embed run --dataset=doj-1 Automation Script See scripts/ingest-dataset.sh (to be created). DAT/OPT File Format DAT (Concordance) Delimiter: þ (thorn character) Fields: Bates Begin, Bates End, Author, Date Created, Email fields, etc. OPT CSV format Fields: Bates Number, Volume, Path, Is First Page, Page Count Troubleshooting VirtioFS Errors (error 95) Use NFS mounts instead: /mnt/fast-nfs/ Or run operations from devcontainer Image Path Mismatch Database stores absolute paths Ensure EPSTEIN_DATA_ROOT matches storage driver config Related docviewer-project-status-recovery-plan - Current project status garage-s3-object-storage - S3 configuration voidlabs-storage-architecture - Storage tiers"
    },
    {
      "id": "projects/docviewer/docviewer-project",
      "title": "DocViewer Project",
      "tags": "index docviewer project",
      "content": "DocViewer Project Document investigation platform for journalists analyzing EFTA (Epstein Files Transparency Act) documents. Features full-text + semantic search, hybrid ranking, OCR support, and multi-dataset isolation. Documentation Core Documentation overview - Project overview, architecture, and technology stack status-and-recovery-plan - Current deployment status and recovery roadmap dataset-ingestion-workflow - How to ingest new document datasets Related Infrastructure garage-s3-object-storage - S3 storage for document images virtiofs-limitations-and-troubleshooting - Storage access patterns Quick Links Resource Location Code /srv/fast/code/epstein/ Worktree .worktrees/multiple-data-sets/ Data /srv/fast/datashare/2025-12-19-Epstein/ Production https://docviewer.voidlabs.cc Beads Tracking Issue Description epstein-rqh Production Recovery Epic epstein-crq Backend deployment fix epstein-qms Frontend deployment fix epstein-4ak Projects feature Status Summary Component Status Backend ❌ Down (Bad Gateway) Frontend ❌ Not deployed Database ✅ Up S3 Storage ⚠️ Partial OCR 5% complete Embeddings 0% complete"
    },
    {
      "id": "projects/docviewer/overview",
      "title": "DocViewer (Epstein Project)",
      "tags": "project document-search semantic-search fastapi react",
      "content": "DocViewer (Epstein Project) Document investigation platform for journalists with full-text + semantic search, hybrid ranking, and OCR support. Inspired by Google Pinpoint. Repository Location: /srv/fast/code/epstein Stack Overview ┌─────────────────────────────────────────────────────────────────┐ │ FRONTEND │ │ Vite + React + TypeScript + Tailwind │ └─────────────────────────────────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────────┐ │ BACKEND │ │ FastAPI + SQLAlchemy + Pydantic │ │ │ │ ┌─────────────┐ ┌─────────────┐ ┌─────────────────────────┐ │ │ │ SearchSvc │ │ EmbeddingSvc│ │ VectorStoreSvc │ │ │ │ (orchestr.) │ │ (Voyage) │ │ (Qdrant) │ │ │ └─────────────┘ └─────────────┘ └─────────────────────────┘ │ └─────────────────────────────────────────────────────────────────┘ │ ┌───────────────────┼───────────────────┐ ▼ ▼ ▼ ┌──────────────────┐ ┌──────────────────┐ ┌──────────────────┐ │ PostgreSQL │ │ Qdrant │ │ File Storage │ │ + ParadeDB BM25 │ │ (Vectors) │ │ (Local/S3) │ └──────────────────┘ └──────────────────┘ └──────────────────┘ Key Decisions Decision Choice Rationale Vector store Qdrant Better filtered ANN than pgvector FTS ParadeDB BM25 PostgreSQL-native, tuneable, IDF-aware Hybrid fusion App-side RRF More control than in-DB fusion Embedding Voyage context-3 84.8% recall on gold set CLI Commands # Backend development cd backend uv sync uv run uvicorn docviewer.main:app --reload --port 8000 # Frontend development cd frontend npm install &amp;&amp; npm run dev # Database migrations uv run docviewer-migrate run # Embedding operations uv run docviewer-embed reindex --only-missing uv run docviewer-embed query &quot;describe the email...&quot; # OCR operations uv run docviewer-ocr import-json ./payload.json # Ingestion uv run docviewer-ingest import-production --data-dir /path/to/data Configuration System .env.defaults # Base config (committed) .env.development # Dev overrides (committed) .env.staging # Staging overrides .env.production # Production overrides .env # Secrets only (git-ignored) - from Phase Precedence (highest to lowest): Environment variables (Phase shell export) .env (secrets) .env.{ENVIRONMENT} (environment-specific) .env.defaults (base config) Deployment (Dokploy) Project Contents External Ports docviewer-dev Qdrant + Postgres 5433, 6334 docviewer-prod Full stack Internal only URLs Dev Postgres: postgresql://epstein:changeme_dev@dokploy.voidlabs.cc:5433/epstein Dev Qdrant: http://dokploy.voidlabs.cc:6334 GPU Processing (Hyperion) Local GPU Embeddings Qwen3-embeddings via llama.cpp on RTX 4080 Super Endpoint: http://hyperion:8080/v1/embeddings Provider: EMBEDDING_PROVIDER=text_embeddings_inference GPU PaddleOCR Typer command: uv run docviewer-ocr paddle-worker Docker image: docker/ocr/Dockerfile.gpu See: docs/hyperion-ocr.md Directory Structure backend/ ├── src/docviewer/ │ ├── api/v1/endpoints/ # FastAPI routes │ ├── cli/ # Typer CLI │ ├── services/ # Business logic │ ├── schemas/ # Pydantic models │ └── core/ # Config, DB setup frontend/ ├── src/ │ ├── components/ │ ├── pages/ │ └── services/ # API client docs/ ├── ARCHITECTURE.md ├── ROADMAP.md └── project-goals.md Current Status ✅ Hybrid search (BM25 + semantic + RRF) ✅ Voyage context-3 embeddings ✅ Qdrant vector store 🔜 Query understanding (typo correction, entity expansion) 🔜 NER at ingest time 🔜 Multi-collection support Related Entries Voidlabs Infrastructure Overview Hyperion GPU Server"
    },
    {
      "id": "projects/tg-scrape/tg-scrape-social-media-astroturfing-analysis",
      "title": "TG Scrape - Social Media Astroturfing Analysis",
      "tags": "project tg-scrape apify facebook scraping astroturfing sqlite investigation",
      "content": "TG Scrape - Social Media Astroturfing Analysis Overview Investigation project to analyze potential astroturfing patterns in social media responses to political posts. Source data is a ~10MB PDF (&quot;TG monitoring binder.pdf&quot;) containing 810 pages of printed emails with social media links that were being monitored by a politician's team. Project Goals Extract all social media links from the PDF Store in SQLite database with proper schema Use Apify to scrape posts and comments from platforms Track post types and author patterns Analyze for astroturfing indicators (repeat actors, timing patterns, coordinated behavior) Current Status Completed PDF Extraction : 11,717 unique links extracted (26,918 total, many duplicates) Database Schema : SQLite with tables for links, posts, replies, authors Facebook Posts : 1,023 of 1,186 posts scraped (86% success rate) Test Comments : 1,402 comments from early test batch Link Breakdown by Platform Platform Links Status Twitter/X 5,170 Not scraped YouTube 3,184 Not scraped Facebook 1,862 1,023 posts scraped Reddit 1,409 Not scraped Instagram 71 Not scraped TikTok 21 Not scraped Facebook Link Types 1,186 posts (scraped) 491 videos/reels (unknown type) 75 profile/page links 67 group links 39 video links 4 photo links Technical Implementation Key Files extract_links.py - PDF text extraction with pypdf, URL classification schema.sql - SQLite schema with links, posts, replies, authors tables db.py - Database utilities and connection management scraper.py - Apify integration and cost estimation store_results.py - Store scraped data in SQLite run_scrape_fast.py - Parallel batch scraper (5 concurrent Apify runs) Database Schema Highlights -- Key analysis view CREATE VIEW frequent_commenters AS SELECT platform, author_username, COUNT(DISTINCT post_id) as posts_commented_on, COUNT(*) as total_comments FROM replies WHERE author_username IS NOT NULL GROUP BY platform, author_username HAVING COUNT(DISTINCT post_id) &gt; 1 ORDER BY posts_commented_on DESC; Apify Integration Actor: apify/facebook-comments-scraper Pricing: $0.006/start + $0.0025/comment Parallel execution: 5 batches concurrent (~175 posts/min) API token stored in scripts (not committed) Next Steps Get Facebook comment counts - Need to determine which posts have high engagement before bulk scraping Scrape Facebook comments - Full comment scrape for astroturfing analysis (~$25 for 10k comments) Twitter/X scraping - 5,170 links (largest dataset) Build analysis queries - Identify repeat commenters, timing patterns, coordinated behavior Budget $40 Apify credits available Current spend: ~$5-8 (test batches + metadata scrape) Remaining: ~$32-35 Resources Decodo residential proxy: 2GB available Decodo ISP static residential: 100GB (10 IPs)"
    },
    {
      "id": "development/memex/markdown-it-py-ast-parser",
      "title": "Markdown-it-py AST Parser",
      "tags": "memex markdown parsing ast markdown-it-py",
      "content": "Markdown-it-py AST Parser Memex uses markdown-it-py for AST-based markdown parsing, replacing the original regex-based approach. Why markdown-it-py? The original implementation used hand-rolled regex patterns for: Link extraction ( \\[\\[([^\\]]+)\\]\\] ) HTML conversion (headers, code blocks, lists, etc.) This caused issues: Missing table support - GFM tables weren't rendered False positives - Links inside code blocks were incorrectly extracted Maintenance burden - Each markdown feature needed its own regex markdown-it-py provides: Proper AST parsing with token stream GFM table support out of the box Plugin architecture for custom syntax Battle-tested CommonMark compliance Wikilink Plugin Custom inline rule for [[wikilink]] syntax: # Pattern: [[target]] or [[target|alias]] WIKILINK_PATTERN = re.compile(r&quot;\\[\\[([^|\\]\\n]+)(?:\\|([^\\]\\n]+))?\\]\\]&quot;) def _wikilink_rule(state: StateInline, silent: bool) -&gt; bool: # Quick rejection test if state.src[state.pos : state.pos + 2] != &quot;[[&quot;: return False match = WIKILINK_PATTERN.match(state.src, state.pos) if not match: return False if not silent: token = state.push(&quot;wikilink&quot;, &quot;&quot;, 0) token.meta = {&quot;target&quot;: match.group(1), &quot;alias&quot;: match.group(2)} state.pos = match.end() return True Key design choices: Quick rejection - Check for [[ before running regex Token metadata - Store target and alias separately Silent mode - Support validation without token emission Single-Pass Architecture The render_markdown() function returns both HTML and links: @dataclass class MarkdownResult: html: str links: list[str] def render_markdown(content: str) -&gt; MarkdownResult: tokens = md.parse(content) links = _extract_wikilinks(tokens) html = md.render(content) return MarkdownResult(html=html, links=links) Benefits: Parse once, extract multiple outputs Links in code blocks aren't extracted (AST knows context) Consistent behavior between HTML and link extraction Usage from memex.parser import render_markdown, extract_links # Full rendering with links result = render_markdown(&quot;See [[foo/bar|docs]] for details.&quot;) print(result.html) # &lt;p&gt;See &lt;a class=&quot;wikilink&quot; data-path=&quot;foo/bar&quot;&gt;docs&lt;/a&gt;...&lt;/p&gt; print(result.links) # ['foo/bar'] # Link extraction only (delegates to render_markdown internally) links = extract_links(&quot;Check [[overview]] and [[details]].&quot;) # ['overview', 'details'] Files File Purpose parser/md_renderer.py markdown-it-py wrapper, wikilink plugin parser/links.py extract_links() delegates to AST parser webapp/api.py Uses render_markdown() for entry HTML Related Memex Knowledge Base - The MCP server using this parser"
    }
  ],
  "metadata": {
    "best-practices/ai-slop-antipatterns": {
      "title": "AI Slop Antipatterns",
      "tags": [
        "ai",
        "code-quality",
        "best-practices",
        "antipatterns",
        "llm",
        "writing",
        "code-review",
        "documentation"
      ],
      "path": "best-practices/ai-slop-antipatterns.html"
    },
    "tooling/claude-code-local-plugin-hooks-bug": {
      "title": "Claude Code Local Plugin Hooks Bug",
      "tags": [
        "tooling",
        "claude-code",
        "plugins",
        "hooks",
        "bugs",
        "workarounds"
      ],
      "path": "tooling/claude-code-local-plugin-hooks-bug.html"
    },
    "tooling/beads-issue-tracker": {
      "title": "Beads Issue Tracker",
      "tags": [
        "tooling",
        "issue-tracking",
        "ai-agents",
        "git"
      ],
      "path": "tooling/beads-issue-tracker.html"
    },
    "tooling/voidlabs-devtools": {
      "title": "Voidlabs Devtools",
      "tags": [
        "tooling",
        "devcontainer",
        "developer-experience",
        "project"
      ],
      "path": "tooling/voidlabs-devtools.html"
    },
    "tooling/vl-mail-agent-messaging": {
      "title": "vl-mail Agent Messaging",
      "tags": [
        "tooling",
        "ai-agents",
        "messaging",
        "beads",
        "cross-project",
        "voidlabs-devtools"
      ],
      "path": "tooling/vl-mail-agent-messaging.html"
    },
    "tooling/vl-mail-lightweight-agent-mail-cli": {
      "title": "vl-mail: Lightweight Agent Mail CLI",
      "tags": [
        "tooling",
        "agent-mail",
        "beads",
        "go",
        "cli",
        "multi-agent"
      ],
      "path": "tooling/vl-mail-lightweight-agent-mail-cli.html"
    },
    "tooling/project-template-sync-pbs": {
      "title": "Project Template Sync (pbs)",
      "tags": [
        "tooling",
        "devcontainer",
        "workflow"
      ],
      "path": "tooling/project-template-sync-pbs.html"
    },
    "projects/dictaphone-inbox-pattern-feature-plan": {
      "title": "Dictaphone Inbox Pattern Feature Plan",
      "tags": [
        "dictaphone",
        "feature",
        "plan",
        "inbox",
        "auto-organize"
      ],
      "path": "projects/dictaphone-inbox-pattern-feature-plan.html"
    },
    "projects/test-beads-full-project": {
      "title": "Test Beads Integration - Full Project",
      "tags": [
        "test",
        "beads"
      ],
      "path": "projects/test-beads-full-project.html"
    },
    "projects/dictaphone-phase-7-8-implementation": {
      "title": "Dictaphone Phase 7-8 Implementation",
      "tags": [
        "dictaphone",
        "swift",
        "ios",
        "macos",
        "phase7",
        "phase8"
      ],
      "path": "projects/dictaphone-phase-7-8-implementation.html"
    },
    "projects/tg-scrape-social-media-astroturfing-analysis-project": {
      "title": "TG Scrape: Social Media Astroturfing Analysis Project",
      "tags": [
        "tg-scrape",
        "apify",
        "facebook",
        "astroturfing",
        "sqlite",
        "social-media-analysis"
      ],
      "path": "projects/tg-scrape-social-media-astroturfing-analysis-project.html"
    },
    "projects/concordance-datopt-import-patterns": {
      "title": "Concordance DAT/OPT Import Patterns",
      "tags": [
        "docviewer",
        "concordance",
        "legal-discovery",
        "data-import",
        "dat-opt",
        "s3"
      ],
      "path": "projects/concordance-datopt-import-patterns.html"
    },
    "projects/dictaphone-phase-5-core-ui-implementation": {
      "title": "Dictaphone Phase 5 - Core UI Implementation",
      "tags": [
        "projects",
        "dictaphone",
        "swiftui",
        "ui"
      ],
      "path": "projects/dictaphone-phase-5-core-ui-implementation.html"
    },
    "projects/cli-tool-path-lookup-behavior": {
      "title": "CLI Tool PATH Lookup Behavior",
      "tags": [
        "focusgroup",
        "configuration",
        "troubleshooting"
      ],
      "path": "projects/cli-tool-path-lookup-behavior.html"
    },
    "projects/docviewer-naming-conventions": {
      "title": "DocViewer Naming Conventions",
      "tags": [
        "docviewer",
        "naming",
        "conventions",
        "documentation"
      ],
      "path": "projects/docviewer-naming-conventions.html"
    },
    "projects/dictaphone-phase-6-cloud-providers-implementation": {
      "title": "Dictaphone Phase 6 - Cloud Providers Implementation",
      "tags": [
        "dictaphone",
        "phase-6",
        "ios",
        "macos",
        "swift",
        "cloud",
        "openai",
        "anthropic"
      ],
      "path": "projects/dictaphone-phase-6-cloud-providers-implementation.html"
    },
    "projects/docviewer-deployment-architecture": {
      "title": "DocViewer Deployment Architecture",
      "tags": [
        "docviewer",
        "deployment",
        "docker",
        "dokploy",
        "architecture"
      ],
      "path": "projects/docviewer-deployment-architecture.html"
    },
    "projects/focusgroup-project": {
      "title": "Focusgroup Project",
      "tags": [
        "projects",
        "focusgroup",
        "agent-tooling"
      ],
      "path": "projects/focusgroup-project.html"
    },
    "projects/dictaphone-app-ios-and-macos-voice-categorization-app": {
      "title": "Dictaphone App - iOS and macOS Voice Categorization App",
      "tags": [
        "projects",
        "ios",
        "macos",
        "swift",
        "swiftui",
        "dictaphone",
        "ai",
        "multiplatform"
      ],
      "path": "projects/dictaphone-app-ios-and-macos-voice-categorization-app.html"
    },
    "projects/sightline-5ightline": {
      "title": "SightLine (5ightline Project)",
      "tags": [
        "project",
        "facial-recognition",
        "aws-rekognition",
        "fastapi",
        "react",
        "lambda"
      ],
      "path": "projects/sightline-5ightline.html"
    },
    "projects/test-beads-single-issue": {
      "title": "Test Beads Integration - Single Issue",
      "tags": [
        "test",
        "beads"
      ],
      "path": "projects/test-beads-single-issue.html"
    },
    "projects/docviewer-local-development-workflow": {
      "title": "DocViewer Local Development Workflow",
      "tags": [
        "docviewer",
        "development",
        "workflow",
        "local-dev",
        "s3"
      ],
      "path": "projects/docviewer-local-development-workflow.html"
    },
    "projects/dictaphone-implementation-progress": {
      "title": "Dictaphone - Implementation Progress",
      "tags": [
        "projects",
        "dictaphone",
        "swift",
        "implementation"
      ],
      "path": "projects/dictaphone-implementation-progress.html"
    },
    "devops/voidlabs-cicd-automation": {
      "title": "Voidlabs CI/CD Automation",
      "tags": [
        "cicd",
        "github-actions",
        "automation",
        "testing",
        "releases"
      ],
      "path": "devops/voidlabs-cicd-automation.html"
    },
    "devops/voidlabs-provisioning-workflow": {
      "title": "Voidlabs Provisioning Workflow",
      "tags": [
        "provisioning",
        "ansible",
        "terraform",
        "workflow",
        "infrastructure"
      ],
      "path": "devops/voidlabs-provisioning-workflow.html"
    },
    "infrastructure/sunshine-appsjson-presets-for-multi-device-resolution": {
      "title": "Sunshine Apps.json Presets for Multi-Device Resolution",
      "tags": [
        "sunshine",
        "moonlight",
        "apps.json",
        "resolution",
        "ipad",
        "macbook",
        "hyprland",
        "headless"
      ],
      "path": "infrastructure/sunshine-appsjson-presets-for-multi-device-resolution.html"
    },
    "infrastructure/memex-knowledge-base": {
      "title": "Memex Knowledge Base",
      "tags": [
        "infrastructure",
        "plugins",
        "documentation",
        "claude-code",
        "mcp",
        "memex"
      ],
      "path": "infrastructure/memex-knowledge-base.html"
    },
    "infrastructure/voidlabs-infrastructure-overview": {
      "title": "Voidlabs Infrastructure Overview",
      "tags": [
        "infrastructure",
        "proxmox",
        "homelab",
        "ansible",
        "terraform",
        "architecture"
      ],
      "path": "infrastructure/voidlabs-infrastructure-overview.html"
    },
    "infrastructure/ansible-inventory-split-guide": {
      "title": "Ansible Inventory Split Guide",
      "tags": [
        "caddy",
        "infrastructure",
        "dns",
        "homelab",
        "docker"
      ],
      "path": "infrastructure/ansible-inventory-split-guide.html"
    },
    "infrastructure/omarchy-vm-setup-on-proxmox-with-gpu-passthrough": {
      "title": "Omarchy VM Setup on Proxmox with GPU Passthrough",
      "tags": [
        "proxmox",
        "omarchy",
        "gpu-passthrough",
        "hyperion"
      ],
      "path": "infrastructure/omarchy-vm-setup-on-proxmox-with-gpu-passthrough.html"
    },
    "infrastructure/virtiofs-limitations-and-troubleshooting": {
      "title": "VirtioFS Limitations and Troubleshooting",
      "tags": [
        "infrastructure",
        "virtiofs",
        "nfs",
        "troubleshooting",
        "proxmox",
        "storage"
      ],
      "path": "infrastructure/virtiofs-limitations-and-troubleshooting.html"
    },
    "infrastructure/proxmox-vm-filesystem-recovery-with-fsck": {
      "title": "Proxmox VM Filesystem Recovery with fsck",
      "tags": [
        "proxmox",
        "filesystem",
        "recovery",
        "fsck",
        "ext4",
        "troubleshooting",
        "vm",
        "read-only",
        "journal",
        "disk-repair",
        "qm",
        "zfs",
        "zvol",
        "e2fsck",
        "corruption"
      ],
      "path": "infrastructure/proxmox-vm-filesystem-recovery-with-fsck.html"
    },
    "infrastructure/sunshine-streaming-setup-on-omarchy-vm-with-nvidia-gpu-passthrough": {
      "title": "Sunshine Streaming Setup on Omarchy VM with NVIDIA GPU Passthrough",
      "tags": [
        "sunshine",
        "moonlight",
        "nvidia",
        "gpu-passthrough",
        "proxmox",
        "omarchy",
        "streaming",
        "wayland",
        "hyprland",
        "headless"
      ],
      "path": "infrastructure/sunshine-streaming-setup-on-omarchy-vm-with-nvidia-gpu-passthrough.html"
    },
    "infrastructure/common-patterns": {
      "title": "Voidlabs Common Patterns",
      "tags": [
        "infrastructure",
        "patterns",
        "best-practices",
        "developer-guide"
      ],
      "path": "infrastructure/common-patterns.html"
    },
    "infrastructure/voidlabs-storage-architecture": {
      "title": "Voidlabs Storage Architecture",
      "tags": [
        "storage",
        "zfs",
        "nfs",
        "virtiofs",
        "infrastructure"
      ],
      "path": "infrastructure/voidlabs-storage-architecture.html"
    },
    "infrastructure/hyperion-proxmox-boot-failure-postmortem": {
      "title": "Hyperion Proxmox Installation Boot Failure Postmortem",
      "tags": [
        "proxmox",
        "hyperion",
        "infrastructure",
        "troubleshooting",
        "postmortem"
      ],
      "path": "infrastructure/hyperion-proxmox-boot-failure-postmortem.html"
    },
    "infrastructure/proxmox-subscription-nag-removal": {
      "title": "Proxmox Subscription Nag Removal",
      "tags": [
        "proxmox",
        "configuration"
      ],
      "path": "infrastructure/proxmox-subscription-nag-removal.html"
    },
    "infrastructure/garage-s3-object-storage": {
      "title": "Garage S3 Object Storage",
      "tags": [
        "infrastructure",
        "s3",
        "garage",
        "object-storage",
        "dokploy"
      ],
      "path": "infrastructure/garage-s3-object-storage.html"
    },
    "infrastructure/proxmox-gpu-passthrough-on-amd-kernel-68-iommu-issue": {
      "title": "Proxmox GPU Passthrough on AMD - Kernel 6.8+ IOMMU Issue",
      "tags": [
        "proxmox",
        "gpu-passthrough",
        "amd",
        "troubleshooting"
      ],
      "path": "infrastructure/proxmox-gpu-passthrough-on-amd-kernel-68-iommu-issue.html"
    },
    "infrastructure/voidlabs-networking-and-dns": {
      "title": "Voidlabs Networking and DNS",
      "tags": [
        "networking",
        "dns",
        "dhcp",
        "caddy",
        "opnsense",
        "infrastructure"
      ],
      "path": "infrastructure/voidlabs-networking-and-dns.html"
    },
    "infrastructure/dokploy/dokploy-vm-crash-recovery-and-traefik-troubleshooting": {
      "title": "Dokploy VM Crash Recovery and Traefik Troubleshooting",
      "tags": [
        "dokploy",
        "traefik",
        "vm-crash",
        "troubleshooting",
        "docker",
        "nfs",
        "systemd",
        "proxmox",
        "503-error",
        "connection-refused",
        "container-not-running",
        "restart-policy",
        "persistent-logging",
        "journald",
        "boot-loop",
        "recovery",
        "infrastructure",
        "vm-restart"
      ],
      "path": "infrastructure/dokploy/dokploy-vm-crash-recovery-and-traefik-troubleshooting.html"
    },
    "infrastructure/dokploy/dokploy-service-naming-and-database-url": {
      "title": "Dokploy Service Naming and DATABASE_URL",
      "tags": [
        "dokploy",
        "database",
        "docker-swarm",
        "configuration",
        "troubleshooting",
        "postgresql",
        "database-url",
        "service-discovery",
        "dns",
        "connection-error",
        "host-not-found",
        "name-resolution"
      ],
      "path": "infrastructure/dokploy/dokploy-service-naming-and-database-url.html"
    },
    "infrastructure/dokploy/dokploy-compose-deployment-lessons": {
      "title": "Dokploy Compose Deployment Lessons",
      "tags": [
        "dokploy",
        "networking",
        "compose",
        "traefik",
        "isolated-deployments",
        "troubleshooting",
        "reference"
      ],
      "path": "infrastructure/dokploy/dokploy-compose-deployment-lessons.html"
    },
    "infrastructure/dokploy/dokploy-container-management": {
      "title": "Dokploy Container Management",
      "tags": [
        "dokploy",
        "docker",
        "containers",
        "debugging",
        "infrastructure"
      ],
      "path": "infrastructure/dokploy/dokploy-container-management.html"
    },
    "infrastructure/dokploy/deployment-guide": {
      "title": "Dokploy Deployment Guide",
      "tags": [
        "dokploy",
        "deployment",
        "api",
        "docker",
        "infrastructure"
      ],
      "path": "infrastructure/dokploy/deployment-guide.html"
    },
    "infrastructure/dokploy/docker-cp-permission-patterns-in-dokploy": {
      "title": "Docker CP Permission Patterns in Dokploy",
      "tags": [
        "dokploy",
        "docker",
        "permissions",
        "troubleshooting",
        "containers"
      ],
      "path": "infrastructure/dokploy/docker-cp-permission-patterns-in-dokploy.html"
    },
    "infrastructure/dokploy/dokploy-network-recovery-after-vm-restart": {
      "title": "Dokploy Network Recovery After VM Restart",
      "tags": [
        "dokploy",
        "traefik",
        "networking",
        "troubleshooting",
        "docker",
        "recovery",
        "504-error",
        "gateway-timeout",
        "isolated-network",
        "vm-restart",
        "docker-compose",
        "bad-gateway"
      ],
      "path": "infrastructure/dokploy/dokploy-network-recovery-after-vm-restart.html"
    },
    "infrastructure/dokploy/dokploy-python-cli-reference": {
      "title": "Dokploy Python CLI Reference",
      "tags": [
        "dokploy",
        "cli",
        "python",
        "api",
        "deployment",
        "reference"
      ],
      "path": "infrastructure/dokploy/dokploy-python-cli-reference.html"
    },
    "projects/docviewer/status-and-recovery-plan": {
      "title": "DocViewer Project Status & Recovery Plan",
      "tags": [
        "docviewer",
        "efta",
        "epstein",
        "deployment",
        "status"
      ],
      "path": "projects/docviewer/status-and-recovery-plan.html"
    },
    "projects/docviewer/docviewer-production-database-access": {
      "title": "DocViewer Production Database Access",
      "tags": [
        "docviewer",
        "database",
        "production",
        "dokploy",
        "postgresql",
        "troubleshooting"
      ],
      "path": "projects/docviewer/docviewer-production-database-access.html"
    },
    "projects/docviewer/dataset-ingestion-workflow": {
      "title": "DocViewer Dataset Ingestion Workflow",
      "tags": [
        "docviewer",
        "ingestion",
        "workflow",
        "datasets",
        "s3"
      ],
      "path": "projects/docviewer/dataset-ingestion-workflow.html"
    },
    "projects/docviewer/docviewer-project": {
      "title": "DocViewer Project",
      "tags": [
        "index",
        "docviewer",
        "project"
      ],
      "path": "projects/docviewer/docviewer-project.html"
    },
    "projects/docviewer/overview": {
      "title": "DocViewer (Epstein Project)",
      "tags": [
        "project",
        "document-search",
        "semantic-search",
        "fastapi",
        "react"
      ],
      "path": "projects/docviewer/overview.html"
    },
    "projects/tg-scrape/tg-scrape-social-media-astroturfing-analysis": {
      "title": "TG Scrape - Social Media Astroturfing Analysis",
      "tags": [
        "project",
        "tg-scrape",
        "apify",
        "facebook",
        "scraping",
        "astroturfing",
        "sqlite",
        "investigation"
      ],
      "path": "projects/tg-scrape/tg-scrape-social-media-astroturfing-analysis.html"
    },
    "development/memex/markdown-it-py-ast-parser": {
      "title": "Markdown-it-py AST Parser",
      "tags": [
        "memex",
        "markdown",
        "parsing",
        "ast",
        "markdown-it-py"
      ],
      "path": "development/memex/markdown-it-py-ast-parser.html"
    }
  }
}